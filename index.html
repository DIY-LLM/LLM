<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DIY LLM Toolkit | Modular Prototype</title>
    
    <link href="https://fonts.googleapis.com/css?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    
    <!-- TensorFlow.js and Chart.js for ML and visualization -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@3.7.1/dist/chart.min.js"></script>
    
    <!-- Using AWS SDK for the mocked S3 functionality (kept for compatibility) -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/aws-sdk/2.1520.0/aws-sdk.min.js"></script>

    <style>
        /* --- Google Cloud/Material Design Variables --- */
        :root {
            --primary-blue: #1A73E8;
            --header-blue: #1A2E44;
            --nav-panel-color: #F1F3F4;
            --active-item-color: #E8F0FE;
            --surface-color: #FFFFFF;
            --background-color: #F8F9FA;
            --text-color: #202124;
            --secondary-text: #5F6368;
            --google-red: #EA4335;
            --google-green: #34A853;
            --google-yellow: #FBBC04;
            --border-color: #DADCE0;
            --shadow-color: rgba(60, 64, 67, 0.1);
        }

        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            background-color: var(--background-color);
            color: var(--text-color);
            display: flex;
            height: 100vh;
            overflow: hidden;
        }

        /* --- Layout --- */
        .container {
            display: flex;
            width: 100%;
        }

        .nav-panel {
            width: 280px;
            background-color: var(--nav-panel-color);
            padding: 20px;
            box-shadow: 2px 0 5px var(--shadow-color);
            overflow-y: auto;
            flex-shrink: 0;
        }

        .content-area {
            flex-grow: 1;
            padding: 20px;
            overflow-y: auto;
        }

        /* --- UI Elements --- */
        h1 {
            color: var(--header-blue);
            font-size: 1.8em;
            font-weight: 500;
            margin-top: 0;
            border-bottom: 2px solid var(--primary-blue);
            padding-bottom: 10px;
            margin-bottom: 20px;
        }

        h2 {
            color: var(--primary-blue);
            font-size: 1.4em;
            font-weight: 400;
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .card {
            background-color: var(--surface-color);
            border-radius: 8px;
            box-shadow: 0 1px 3px var(--shadow-color);
            padding: 20px;
            margin-bottom: 20px;
            border: 1px solid var(--border-color);
        }

        textarea, input[type="text"], input[type="number"], select {
            width: 100%;
            padding: 10px;
            margin-bottom: 10px;
            border: 1px solid var(--border-color);
            border-radius: 4px;
            box-sizing: border-box;
            font-size: 1em;
            font-family: inherit;
        }

        .button {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            padding: 10px 15px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.2s ease;
            margin-right: 10px;
            margin-bottom: 10px;
        }

        .button-primary {
            background-color: var(--primary-blue);
            color: var(--surface-color);
        }

        .button-primary:hover {
            background-color: #1565C0;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
        }

        .button-secondary {
            background-color: var(--nav-panel-color);
            color: var(--text-color);
            border: 1px solid var(--border-color);
        }

        .button-secondary:hover {
            background-color: #E6E9EA;
        }

        .button-red {
            background-color: var(--google-red);
            color: var(--surface-color);
        }

        .button-red:hover {
            background-color: #D32F2F;
        }

        .label {
            display: block;
            margin-bottom: 5px;
            font-weight: 500;
            color: var(--secondary-text);
        }

        /* --- Training Status Indicator --- */
        #worker-status {
            font-weight: 700;
            padding: 5px 10px;
            border-radius: 4px;
            display: inline-block;
            margin-left: 10px;
        }

        .status-ready {
            background-color: #E6F3E9; /* Light green */
            color: var(--google-green);
        }

        .status-initializing, .status-loading {
            background-color: #FFFBE5; /* Light yellow */
            color: var(--google-yellow);
        }

        .status-training {
            background-color: #E8F0FE; /* Light blue */
            color: var(--primary-blue);
        }
        
        .status-error {
            background-color: #FEEBE7; /* Light red */
            color: var(--google-red);
        }
        
        /* --- Alert Box --- */
        #alert-box {
            position: fixed;
            top: 20px;
            right: 20px;
            z-index: 1000;
            background-color: var(--surface-color);
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
            padding: 15px;
            border-left: 5px solid var(--primary-blue);
            max-width: 350px;
            display: none;
            transition: all 0.3s ease;
        }

        #alert-box.show {
            display: block;
            opacity: 1;
        }

        #alert-box-title {
            font-weight: 700;
            margin-bottom: 5px;
            color: var(--primary-blue);
        }

        #alert-box-message {
            font-size: 0.9em;
        }
        
        /* --- Chart Container --- */
        #loss-chart-container {
            height: 250px;
            width: 100%;
        }

        /* --- Responsive Design --- */
        @media (max-width: 768px) {
            .container {
                flex-direction: column;
            }
            .nav-panel {
                width: 100%;
                height: auto;
                max-height: 50vh;
                order: 2; /* Move to the bottom for mobile, or keep on top */
            }
            .content-area {
                order: 1;
                padding: 15px;
            }
            .card {
                padding: 15px;
            }
        }
    </style>
</head>
<body>

    <div class="container">
        
        <!-- Navigation/Control Panel -->
        <div class="nav-panel">
            <h1 style="margin-bottom: 10px; border: none;">DIY LLM Toolkit</h1>
            <p style="color: var(--secondary-text); margin-top: 0;">Modular Prototype</p>

            <div class="card">
                <h2>Mode Selection</h2>
                <div style="display: flex; gap: 10px; margin-bottom: 15px;">
                    <button id="mode-markov" class="button button-secondary" onclick="setMode('MARKOV')">
                        <i class="fas fa-random"></i> Markov Chain
                    </button>
                    <button id="mode-transformer" class="button button-primary" onclick="setMode('TRANSFORMER')">
                        <i class="fas fa-brain"></i> Mini Transformer
                    </button>
                </div>
                <div style="font-size: 0.9em;">
                    <strong>Current Mode:</strong> <span id="current-mode">Mini Transformer</span>
                </div>
            </div>

            <div class="card" id="data-card">
                <h2>Data & Tokenization</h2>
                <label class="label" for="training-data">Training Data</label>
                <textarea id="training-data" rows="5" placeholder="Enter your text corpus here..."></textarea>
                
                <div style="display: flex; justify-content: space-between; font-size: 0.9em; color: var(--secondary-text); margin-bottom: 10px;">
                    <span id="token-count">Tokens: 0</span>
                    <span id="vocab-size">Vocab: 0</span>
                </div>
                
                <button class="button button-secondary" onclick="handleDataUpload()">
                    <i class="fas fa-upload"></i> Upload Data
                </button>
                <button class="button button-secondary" onclick="dataHandler.tokenizeData()">
                    <i class="fas fa-code"></i> Tokenize
                </button>
            </div>

            <div class="card" id="transformer-train-card">
                <h2>Transformer Training</h2>
                <div style="margin-bottom: 10px; font-size: 0.9em; color: var(--secondary-text);">
                    <i class="fas fa-circle-notch fa-spin status-initializing" style="font-size: 0.9em;"></i>
                    <strong id="worker-status" class="status-initializing">Status: Initializing</strong>
                </div>

                <div class="grid-2-col" style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                    <div>
                        <label class="label" for="epochs">Epochs</label>
                        <input type="number" id="epochs" value="10" min="1" max="100">
                    </div>
                    <div>
                        <label class="label" for="batch-size">Batch Size</label>
                        <input type="number" id="batch-size" value="16" min="8" max="64">
                    </div>
                </div>

                <button id="train-button" class="button button-primary" onclick="miniTransformer.startTraining(parseInt(document.getElementById('epochs').value), parseInt(document.getElementById('batch-size').value))">
                    <i class="fas fa-play"></i> Start Training
                </button>
                <button id="stop-button" class="button button-red" disabled onclick="miniTransformer.stopTraining()">
                    <i class="fas fa-stop"></i> Stop Training
                </button>
                <button id="delete-checkpoint-button" class="button button-secondary" onclick="miniTransformer.deleteCheckpoint()">
                    <i class="fas fa-trash-alt"></i> Delete Checkpoint
                </button>
                
                <div id="loss-chart-container" style="margin-top: 20px;">
                    <canvas id="loss-chart"></canvas>
                </div>
            </div>
            
             <div class="card" id="markov-train-card" style="display: none;">
                <h2>Markov Training</h2>
                <p style="font-size: 0.9em; color: var(--secondary-text);">
                    <i class="fas fa-info-circle"></i> Training is instantaneous for Markov Chains.
                </p>
                <button class="button button-primary" onclick="markovModel.train(dataHandler.getTokens())">
                    <i class="fas fa-play"></i> Train Markov Model
                </button>
                <div id="markov-status" style="margin-top: 15px; font-size: 0.9em; font-weight: 500; color: var(--secondary-text);">
                    Model Status: Untrained
                </div>
            </div>
        </div>
        
        <!-- Main Content Area -->
        <div class="content-area">
            <h1>LLM Generation</h1>
            
            <div class="card">
                <h2>Prompt & Generation</h2>
                <label class="label" for="prompt-input">Input Prompt (Seed Text)</label>
                <input type="text" id="prompt-input" placeholder="Start your prompt here...">
                
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                    <div style="font-size: 0.9em;">
                        <label class="label" for="temperature-slider">Temperature: <span id="temperature-value">0.8</span></label>
                        <input type="range" id="temperature-slider" min="0.1" max="1.5" step="0.1" value="0.8" oninput="updateTemperature(this.value)" style="width: 150px; margin: 0; vertical-align: middle;">
                    </div>
                    <button class="button button-primary" onclick="handleGeneration()">
                        <i class="fas fa-feather-alt"></i> Generate Text
                    </button>
                </div>
            </div>

            <div class="card">
                <h2>Output</h2>
                <div id="output-area" style="min-height: 150px; background-color: var(--background-color); padding: 10px; border-radius: 4px; border: 1px dashed var(--border-color); color: var(--secondary-text);">
                    Generated text will appear here...
                </div>
            </div>
        </div>
    </div>
    
    <!-- Custom Alert Box -->
    <div id="alert-box">
        <div id="alert-box-title"></div>
        <div id="alert-box-message"></div>
    </div>

    <!-- Hidden Script for Web Worker Content -->
    <!-- FIX 1: Worker script embedded as a template literal string to create a Blob URL -->
    <script id="training-worker-source" type="text/template">
        // training-worker.js
        // Runs in a background thread for non-blocking UI training with IndexedDB persistence
        // ====

        // Import TF.js in the worker thread
        importScripts('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js');

        let CONFIG = {}; // Will be set on 'START'

        let workerModel = null;
        let currentTrainingJob = null;
        let checkpointMetadata = null;
        let tokenizerState = { vocab: new Map(), idToToken: new Map() };
        let isTraining = false;
        let tfjsInitialized = false;

        // IndexedDB Persistence
        // ======================

        function openDB() {
            return new Promise((resolve, reject) => {
                const request = indexedDB.open(CONFIG.DB_NAME, 1);

                request.onupgradeneeded = (event) => {
                    const db = event.target.result;
                    db.createObjectStore(CONFIG.STORE_NAME, { keyPath: 'taskId' });
                };

                request.onsuccess = (event) => resolve(event.target.result);
                request.onerror = (event) => reject(event.target.error);
            });
        }

        async function saveCheckpoint(taskId, epoch, loss, weights) {
            // Check for max size before saving (simplified check)
            if (weights.byteLength / (1024 * 1024) > CONFIG.MAX_CHECKPOINT_SIZE_MB) {
                 console.warn("Checkpoint size exceeds max limit. Skipping save.");
                 return;
            }

            const db = await openDB();
            const tx = db.transaction(CONFIG.STORE_NAME, 'readwrite');
            const store = tx.objectStore(CONFIG.STORE_NAME);

            const record = {
                taskId: taskId,
                epoch: epoch,
                loss: loss,
                timestamp: Date.now(),
                weights: weights // ArrayBuffer of the model weights
            };

            return new Promise((resolve, reject) => {
                const request = store.put(record);
                request.onsuccess = () => resolve();
                request.onerror = (event) => reject(event.target.error);
            });
        }

        async function loadCheckpoint(taskId) {
            const db = await openDB();
            const tx = db.transaction(CONFIG.STORE_NAME, 'readonly');
            const store = tx.objectStore(CONFIG.STORE_NAME);

            return new Promise((resolve, reject) => {
                const request = store.get(taskId);
                request.onsuccess = (event) => resolve(event.target.result);
                request.onerror = (event) => reject(event.target.error);
            });
        }

        async function deleteCheckpoint(taskId) {
            const db = await openDB();
            const tx = db.transaction(CONFIG.STORE_NAME, 'readwrite');
            const store = tx.objectStore(CONFIG.STORE_NAME);

            return new Promise((resolve, reject) => {
                const request = store.delete(taskId);
                request.onsuccess = () => {
                    checkpointMetadata = null; // Clear metadata on successful delete
                    postMessage({ command: 'CHECKPOINT_DELETED' });
                    resolve();
                }
                request.onerror = (event) => reject(event.target.error);
            });
        }

        async function checkExistingCheckpoint() {
            try {
                const data = await loadCheckpoint(CONFIG.taskId);
                if (data) {
                    checkpointMetadata = {
                        epoch: data.epoch,
                        loss: data.loss,
                        timestamp: data.timestamp
                    };
                    postMessage({ command: 'CHECKPOINT_FOUND', metadata: checkpointMetadata });
                } else {
                    checkpointMetadata = null;
                    postMessage({ command: 'CHECKPOINT_NOT_FOUND' });
                }
            } catch (e) {
                console.error("Error checking checkpoint:", e);
                postMessage({ command: 'ERROR', message: `Error checking checkpoint: ${e.message}` });
            }
        }
        
        // Model Definition
        // ======================

        function createModel(vocabSize, sequenceLength) {
            if (workerModel) {
                workerModel.dispose();
            }

            const embeddingDim = 64;
            const headSize = 128; // Smaller head size for mini transformer
            const numHeads = 4;
            const numLayers = 2; // Very shallow transformer

            const input = tf.input({ shape: [sequenceLength] });
            
            let x = tf.layers.embedding({
                inputDim: vocabSize,
                outputDim: embeddingDim,
                inputLength: sequenceLength
            }).apply(input);

            // Add position encoding (simplified/learned)
            // x = tf.layers.positionalEncoding().apply(x); // Not available in TF.js by default, simplified for this prototype

            // Simple Attention/Dense Layers as a substitute for a full Transformer block
            for (let i = 0; i < numLayers; i++) {
                // Simplified multi-head attention (using Dense layers)
                // In a real mini-transformer, this would be a proper Attention layer
                x = tf.layers.dense({ units: headSize, activation: 'relu' }).apply(x);
                x = tf.layers.dense({ units: embeddingDim }).apply(x);
                x = tf.layers.layerNormalization().apply(x);
            }
            
            // Global average pooling (needed to flatten for final Dense layer)
            x = tf.layers.globalAveragePooling1d().apply(x);
            
            // Final prediction layer
            const output = tf.layers.dense({
                units: vocabSize,
                activation: 'softmax'
            }).apply(x);

            workerModel = tf.model({ inputs: input, outputs: output });

            workerModel.compile({
                optimizer: tf.train.adam(CONFIG.train.learningRate),
                loss: 'sparseCategoricalCrossentropy',
                metrics: ['accuracy']
            });
            
            console.log("Mini Transformer Model Created. Layers:", workerModel.layers.length);
        }

        // Training Logic
        // ======================
        
        // Function to convert ArrayBuffer back to an array of TF.Tensors
        function arrayBufferToWeights(buffer, weightsInfo) {
            const data = new Float32Array(buffer); // Assuming Float32, adjust if necessary
            let offset = 0;
            const tensors = [];
            
            for (const info of weightsInfo) {
                const size = info.shape.reduce((a, b) => a * b, 1);
                const tensorData = data.slice(offset, offset + size);
                tensors.push(tf.tensor(tensorData, info.shape, info.dtype));
                offset += size;
            }
            return tensors;
        }

        async function trainWorkerModel(data) {
            if (isTraining) {
                console.log("Already training.");
                return;
            }

            isTraining = true;
            CONFIG = data.config;
            tokenizerState = data.tokenizerState;
            const vocabSize = data.vocabSize;
            const sequenceLength = CONFIG.SEQUENCE_LENGTH;
            const { inputSequences, targetTokens } = data.trainingData;
            
            const epochs = CONFIG.train.epochs;
            const batchSize = CONFIG.train.batchSize;
            let currentEpoch = 0;

            // 1. Create Model
            createModel(vocabSize, sequenceLength);

            // 2. Load Checkpoint if available
            try {
                const checkpoint = await loadCheckpoint(CONFIG.taskId);
                if (checkpoint) {
                    currentEpoch = checkpoint.epoch;
                    
                    // Convert weights ArrayBuffer back to Tensors
                    const weightData = workerModel.getWeights().map(w => ({ name: w.name, dtype: w.dtype, shape: w.shape }));
                    const tensors = arrayBufferToWeights(checkpoint.weights, weightData);
                    
                    workerModel.setWeights(tensors);
                    tensors.forEach(t => t.dispose()); // Clean up loaded tensors
                    console.log(`Model restored from epoch ${currentEpoch}.`);
                }
            } catch (e) {
                console.error("Error loading checkpoint, starting from scratch:", e);
                // Continue with a fresh model
            }

            // 3. Prepare Data
            const xTrain = tf.tensor2d(inputSequences, [inputSequences.length, sequenceLength], 'int32');
            const yTrain = tf.tensor1d(targetTokens, 'int32');

            // 4. Start Training
            try {
                await workerModel.fit(xTrain, yTrain, {
                    epochs: epochs,
                    initialEpoch: currentEpoch, // Start from where we left off
                    batchSize: batchSize,
                    callbacks: {
                        onBatchEnd: (batch, logs) => {
                            if (!isTraining) {
                                workerModel.stopTraining = true;
                                return;
                            }
                            postMessage({ command: 'BATCH_END', batch: batch, logs: logs });
                        },
                        onEpochEnd: async (epoch, logs) => {
                            if (!isTraining) {
                                workerModel.stopTraining = true;
                                return;
                            }
                            // Save checkpoint after each epoch
                            const weightData = await tf.buffer(workerModel.getWeights()[0].shape, 'float32', workerModel.getWeights().map(w => w.dataSync()));

                            // Save checkpoint: taskId, epoch, loss, weight ArrayBuffer
                            await saveCheckpoint(CONFIG.taskId, epoch + 1, logs.loss, weightData.buffer);

                            console.log(`Epoch ${epoch + 1} complete. Loss: ${logs.loss.toFixed(4)}. Checkpoint saved.`);

                            postMessage({ command: 'EPOCH_END', epoch: epoch, logs: logs });
                        }
                    }
                });

                // Only fire COMPLETE if it wasn't stopped manually
                if (isTraining) {
                    postMessage({ command: 'COMPLETE' });
                }

            } catch (e) {
                if (e.message !== 'model.stopTraining is true.') {
                    // Manually stopped, not an error
                    postMessage({ command: 'ERROR', message: e.message });
                    console.error(e);
                }
            } finally {
                isTraining = false;
                xTrain.dispose();
                yTrain.dispose();
                // We keep the model in memory for generation, so we don't dispose of workerModel
            }
        }


        // Generation Logic
        // ======================
        
        async function generateWorkerText(data) {
             if (!workerModel) {
                 postMessage({ command: 'ERROR', message: 'No model loaded in worker for generation.' });
                 return;
             }

             const { inputTokens, sequenceLength, maxNewTokens, temperature, tokenToId, idToToken, UNK_TOKEN_ID } = data;
             let tokens = [...inputTokens];

             for (let i = 0; i < maxNewTokens; i++) {
                 // 1. Get the context sequence (last 'sequenceLength' tokens)
                 const context = tokens.slice(-sequenceLength);
                 
                 // Pad or truncate if needed, though inputTokens should be handled by MiniTransformer class
                 const paddedContext = context.length < sequenceLength 
                    ? [...Array(sequenceLength - context.length).fill(UNK_TOKEN_ID), ...context]
                    : context;

                 // 2. Convert to TF tensor
                 const inputTensor = tf.tensor2d([paddedContext], [1, sequenceLength], 'int32');

                 // 3. Predict next token probabilities
                 const prediction = workerModel.predict(inputTensor); // [1, vocabSize]

                 // 4. Apply temperature and sample the next token
                 const logits = tf.squeeze(prediction, [0]); // [vocabSize]
                 const logitData = logits.dataSync();

                 // Logits to probabilities (with temperature scaling)
                 let scaledLogits = Array.from(logitData).map(logit => logit / temperature);

                 // Softmax to get probabilities
                 const maxLogit = Math.max(...scaledLogits);
                 const expLogits = scaledLogits.map(l => Math.exp(l - maxLogit));
                 const sumExp = expLogits.reduce((a, b) => a + b, 0);
                 const probabilities = expLogits.map(l => l / sumExp);
                 
                 // Sample the next token index
                 let nextTokenIndex = -1;
                 let cumulativeProb = 0;
                 const randomValue = Math.random();

                 for (let j = 0; j < probabilities.length; j++) {
                     cumulativeProb += probabilities[j];
                     if (randomValue < cumulativeProb) {
                         nextTokenIndex = j;
                         break;
                     }
                 }

                 inputTensor.dispose();
                 prediction.dispose();
                 logits.dispose();

                 if (nextTokenIndex !== -1) {
                     tokens.push(nextTokenIndex);
                     postMessage({ command: 'TOKEN_GENERATED', token: idToToken.get(nextTokenIndex) });
                 } else {
                    // Stop if sampling fails
                    break;
                 }

             }
             
             postMessage({ command: 'GENERATION_COMPLETE' });
        }


        // Message Handler
        // ======================

        self.onmessage = async (event) => {
            const data = event.data;

            if (!tfjsInitialized) {
                // Initialize TF.js backend if necessary (not always needed in a dedicated worker)
                await tf.ready();
                tfjsInitialized = true;
                postMessage({ command: 'INIT_READY' });
            }

            switch (data.command) {
                case 'START':
                    trainWorkerModel(data);
                    break;
                case 'STOP':
                    isTraining = false; // Stop the fit loop
                    break;
                case 'CHECK_CHECKPOINT':
                    CONFIG = data.config;
                    checkExistingCheckpoint();
                    break;
                case 'DELETE_CHECKPOINT':
                    CONFIG = data.config;
                    deleteCheckpoint(data.taskId);
                    break;
                case 'LOAD_MODEL_AND_GENERATE':
                    CONFIG = data.config;
                    // Load the model from the checkpoint before generating if not already in memory
                    if (!workerModel) {
                        const checkpoint = await loadCheckpoint(CONFIG.taskId);
                        if (checkpoint) {
                            createModel(data.vocabSize, CONFIG.SEQUENCE_LENGTH);
                            const weightData = workerModel.getWeights().map(w => ({ name: w.name, dtype: w.dtype, shape: w.shape }));
                            const tensors = arrayBufferToWeights(checkpoint.weights, weightData);
                            workerModel.setWeights(tensors);
                            tensors.forEach(t => t.dispose()); 
                            console.log("Model loaded in worker for generation.");
                        } else {
                            postMessage({ command: 'ERROR', message: 'No checkpoint found to load model for generation.' });
                            return;
                        }
                    }
                    generateWorkerText(data);
                    break;
            }
        };
    </script>


    <script>
        // Global Constants
        const CONFIG = {
            // ML Configuration
            SEQUENCE_LENGTH: 10,
            UNK_TOKEN: '<UNK>',
            UNK_TOKEN_ID: 0,
            MAX_NEW_TOKENS: 100,
            temperature: 0.8, // Default generation temperature
            
            // UI/State
            taskId: 'mini-transformer-v1', // Unique ID for IndexedDB
            
            // Worker/Persistence Configuration
            MAX_CHECKPOINT_SIZE_MB: 50, // Max size for weights storage
            DB_NAME: 'LLMTrainingDB',
            STORE_NAME: 'Checkpoints',
            train: {
                epochs: 10, // Default for UI
                batchSize: 16, // Default for UI
                learningRate: 0.0005
            }
        };

        const state = {
            mode: 'TRANSFORMER', // 'MARKOV' or 'TRANSFORMER'
            isTraining: false,
            isWorkerReady: false,
            isCheckpointLoaded: false
        };

        // Global instances (FIX 3: Declaring them globally)
        let dataHandler;
        let markovModel;
        let miniTransformer;
        let lossChart;


        // Utility Functions
        // ==========================================================

        /**
         * FIX 2: Define setWorkerStatus globally for use by MiniTransformer
         * Updates the UI to reflect the status of the training worker.
         * @param {string} status - e.g., 'Initializing', 'Ready', 'Training', 'Error'
         * @param {string} message - A descriptive message
         */
        function setWorkerStatus(status, message = '') {
            const statusElement = document.getElementById('worker-status');
            if (statusElement) {
                statusElement.textContent = `Status: ${status}`;
                // Apply status-specific styling
                statusElement.className = `status-${status.toLowerCase().replace(/\s/g, '-')}`; 
                if (message) {
                    console.log(`[Worker] ${status}: ${message}`);
                }
            }
        }

        /**
         * Displays a custom, non-blocking alert box.
         * @param {string} message - The main alert message.
         * @param {string} title - The title of the alert.
         * @param {number} duration - Duration in milliseconds.
         */
        function showAlert(message, title = 'Notification', duration = 4000) {
            const box = document.getElementById('alert-box');
            document.getElementById('alert-box-title').textContent = title;
            document.getElementById('alert-box-message').textContent = message;

            box.classList.add('show');

            setTimeout(() => {
                box.classList.remove('show');
            }, duration);
        }

        /**
         * Initializes the loss chart using Chart.js.
         */
        function initializeChart() {
            const ctx = document.getElementById('loss-chart').getContext('2d');
            lossChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Training Loss',
                        data: [],
                        borderColor: var(--primary-blue),
                        backgroundColor: 'rgba(26, 115, 232, 0.1)',
                        tension: 0.4,
                        borderWidth: 2,
                        pointRadius: 3
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Epoch / Batch'
                            }
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Loss'
                            },
                            beginAtZero: true
                        }
                    },
                    plugins: {
                        legend: {
                            display: false
                        }
                    }
                }
            });
        }
        
        /**
         * Updates the loss chart with new data points.
         * @param {number} label - The label for the x-axis (e.g., batch number or epoch number).
         * @param {number} loss - The loss value.
         * @param {boolean} isEpochEnd - True if the update is for the end of an epoch.
         */
        function updateLossChart(label, loss, isEpochEnd) {
             const data = lossChart.data.datasets[0].data;
             const labels = lossChart.data.labels;
             
             if (isEpochEnd) {
                 // On epoch end, clear and show only epoch data or a summary
                 // For simplicity, we'll append both batch and epoch data, but reset the view
                 // to focus on the recent points.
                 // For the prototype, we'll just append.
                 lossChart.data.datasets[0].borderColor = var(--primary-blue);
                 lossChart.data.datasets[0].pointRadius = 4;
             } else {
                 lossChart.data.datasets[0].borderColor = 'rgba(26, 115, 232, 0.5)';
                 lossChart.data.datasets[0].pointRadius = 1;
             }
             
             labels.push(label);
             data.push(loss);
             
             // Keep only the last 100 points to prevent memory/performance issues
             const maxPoints = 100;
             if (labels.length > maxPoints) {
                 labels.splice(0, labels.length - maxPoints);
                 data.splice(0, data.length - maxPoints);
             }
             
             lossChart.update('none'); // 'none' for instant update
         }

        /**
         * Resets the loss chart.
         */
        function resetLossChart() {
            lossChart.data.labels = [];
            lossChart.data.datasets[0].data = [];
            lossChart.update();
        }

        /**
         * Toggles the UI elements based on the selected mode.
         * @param {string} newMode - 'MARKOV' or 'TRANSFORMER'
         */
        function setMode(newMode) {
            state.mode = newMode;
            document.getElementById('current-mode').textContent = newMode === 'MARKOV' ? 'Markov Chain' : 'Mini Transformer';
            
            const transformerCard = document.getElementById('transformer-train-card');
            const markovCard = document.getElementById('markov-train-card');
            const modeMarkovBtn = document.getElementById('mode-markov');
            const modeTransformerBtn = document.getElementById('mode-transformer');
            
            if (newMode === 'TRANSFORMER') {
                transformerCard.style.display = 'block';
                markovCard.style.display = 'none';
                modeTransformerBtn.classList.add('button-primary');
                modeTransformerBtn.classList.remove('button-secondary');
                modeMarkovBtn.classList.add('button-secondary');
                modeMarkovBtn.classList.remove('button-primary');
            } else {
                transformerCard.style.display = 'none';
                markovCard.style.display = 'block';
                modeMarkovBtn.classList.add('button-primary');
                modeMarkovBtn.classList.remove('button-secondary');
                modeTransformerBtn.classList.add('button-secondary');
                modeTransformerBtn.classList.remove('button-primary');
            }
        }
        
        /**
         * Updates the temperature value and display.
         * @param {string} value - The temperature slider value.
         */
        function updateTemperature(value) {
            CONFIG.temperature = parseFloat(value);
            document.getElementById('temperature-value').textContent = value;
        }

        // LLM Models and Logic
        // ==========================================================

        /**
         * Handles tokenization, including creating a vocabulary and mapping tokens to IDs.
         */
        class DataHandler {
            constructor(unkToken, unkTokenId) {
                this.text = "";
                this.tokens = [];
                this.vocab = new Map(); // token -> id
                this.idToToken = new Map(); // id -> token
                this.unkToken = unkToken;
                this.unkTokenId = unkTokenId;
            }

            // Getters
            getVocabSize() { return this.vocab.size; }
            getTokens() { return this.tokens; }

            /**
             * Processes the raw text data.
             */
            tokenizeData() {
                this.text = document.getElementById('training-data').value.trim();
                if (this.text.length === 0) {
                    showAlert('Please enter some text in the training data field.', 'Data Required');
                    return;
                }

                // Simple word-level tokenization (splits on spaces and punctuation)
                this.tokens = this.text
                    .toLowerCase()
                    .replace(/[^a-z0-9\s]/g, ' ') // Replace non-alphanumeric/non-space with space
                    .replace(/\s+/g, ' ') // Collapse multiple spaces
                    .trim()
                    .split(' ');
                
                if (this.tokens.length === 0) {
                     showAlert('Tokenization resulted in no tokens. Please check your data.', 'Tokenization Error');
                     return;
                }

                // 1. Build Vocabulary
                this.vocab.clear();
                this.idToToken.clear();

                // Start with UNK token
                this.vocab.set(this.unkToken, this.unkTokenId);
                this.idToToken.set(this.unkTokenId, this.unkToken);

                let idCounter = this.unkTokenId + 1;
                this.tokens.forEach(token => {
                    if (!this.vocab.has(token)) {
                        this.vocab.set(token, idCounter);
                        this.idToToken.set(idCounter, token);
                        idCounter++;
                    }
                });

                // 2. Update UI
                document.getElementById('token-count').textContent = `Tokens: ${this.tokens.length}`;
                document.getElementById('vocab-size').textContent = `Vocab: ${this.getVocabSize()}`;
                showAlert(`Tokenization complete! Total Tokens: ${this.tokens.length}, Vocab Size: ${this.getVocabSize()}`, 'Tokenization Success');
            }

            /**
             * Converts tokens into sequences for Transformer training.
             * @param {number} sequenceLength - Length of input sequences.
             * @returns {{inputSequences: number[][], targetTokens: number[]}}
             */
            createTrainingSequences(sequenceLength) {
                const inputSequences = [];
                const targetTokens = [];
                const tokenIds = this.tokens.map(token => this.vocab.get(token) || this.unkTokenId);

                // Create sequences of size 'sequenceLength' with the next token as the target
                for (let i = 0; i < tokenIds.length - sequenceLength; i++) {
                    const input = tokenIds.slice(i, i + sequenceLength);
                    const target = tokenIds[i + sequenceLength];
                    inputSequences.push(input);
                    targetTokens.push(target);
                }

                return { inputSequences, targetTokens };
            }
        }

        /**
         * Simple Markov Chain Model.
         */
        class MarkovModel {
            constructor() {
                this.transitionMatrix = null;
                this.modelStatusElement = document.getElementById('markov-status');
            }

            train(tokens) {
                if (tokens.length < 2) {
                    showAlert('Training data must have at least two tokens for Markov Chain.', 'Training Error');
                    this.transitionMatrix = null;
                    this.updateUI(false);
                    return;
                }

                this.transitionMatrix = new Map(); // word -> { next_word -> count }

                for (let i = 0; i < tokens.length - 1; i++) {
                    const current = tokens[i];
                    const next = tokens[i + 1];

                    if (!this.transitionMatrix.has(current)) {
                        this.transitionMatrix.set(current, new Map());
                    }

                    const nextWords = this.transitionMatrix.get(current);
                    nextWords.set(next, (nextWords.get(next) || 0) + 1);
                }
                
                this.updateUI(true);
                showAlert('Markov Model training complete.', 'Training Success');
            }

            /**
             * Generates text using the Markov model.
             * @param {string} prompt - The seed word.
             * @param {number} maxNewTokens - Max number of tokens to generate.
             * @returns {string} The generated text.
             */
            generate(prompt, maxNewTokens = CONFIG.MAX_NEW_TOKENS) {
                if (!this.transitionMatrix) {
                    throw new Error("Markov Model is not trained.");
                }

                let currentToken = prompt.toLowerCase().trim().split(' ')[0]; // Use the first token of the prompt as seed
                let generatedTokens = [currentToken];

                for (let i = 0; i < maxNewTokens; i++) {
                    const nextWordsMap = this.transitionMatrix.get(currentToken);

                    if (!nextWordsMap || nextWordsMap.size === 0) {
                        break; // Stop if no transitions exist
                    }

                    // Calculate total count
                    let totalCount = 0;
                    for (const count of nextWordsMap.values()) {
                        totalCount += count;
                    }

                    // Select the next word based on probability
                    let randomValue = Math.random() * totalCount;
                    let cumulativeCount = 0;
                    let nextToken = null;

                    for (const [word, count] of nextWordsMap.entries()) {
                        cumulativeCount += count;
                        if (randomValue < cumulativeCount) {
                            nextToken = word;
                            break;
                        }
                    }

                    if (nextToken) {
                        generatedTokens.push(nextToken);
                        currentToken = nextToken;
                    } else {
                        break; // Should not happen if totalCount is calculated correctly
                    }
                }

                return generatedTokens.join(' ');
            }
            
            updateUI(isTrained) {
                 if (isTrained) {
                     this.modelStatusElement.innerHTML = `Model Status: <span style="color: var(--google-green);">Trained</span>`;
                 } else {
                     this.modelStatusElement.innerHTML = `Model Status: <span style="color: var(--google-red);">Untrained</span>`;
                 }
            }
        }

        /**
         * Mini Transformer Model (Web Worker Interface).
         */
        class MiniTransformer {
            constructor(taskId, config) {
                this.taskId = taskId;
                this.config = config;
                this.isTraining = false;
                this.worker = null;
                this.model = null; // Stored model for immediate generation
                this.epochsTrained = 0;

                this.initializeWorker();
                this.updateUI(false);
            }
            
            /**
             * FIX 1: Creates the Web Worker using a Blob URL from the embedded script source.
             */
            initializeWorker() {
                try {
                    // Get the worker script content from the hidden script tag
                    const workerScriptSource = document.getElementById('training-worker-source').textContent;
                    
                    // Create a Blob from the script content
                    const blob = new Blob([workerScriptSource], { type: 'application/javascript' });
                    
                    // Create a Worker from the Blob URL
                    const workerUrl = URL.createObjectURL(blob);
                    this.worker = new Worker(workerUrl);

                    this.worker.onmessage = this.handleWorkerMessage.bind(this);
                    this.worker.onerror = this.handleWorkerError.bind(this);
                    
                    setWorkerStatus('Initializing', 'Web Worker created successfully.');

                    // Check for existing checkpoint immediately
                    this.checkCheckpoint();

                } catch (e) {
                    this.handleWorkerError(e);
                }
            }

            /**
             * Handles errors from the Web Worker.
             */
            handleWorkerError(event) {
                const errorMessage = event.message || "An unknown worker error occurred.";
                setWorkerStatus('Error', errorMessage);
                showAlert(errorMessage, 'Worker Error');
                this.updateUI(false); // Ensure buttons are clickable
            }

            /**
             * Handles messages coming back from the Web Worker.
             */
            handleWorkerMessage(event) {
                const data = event.data;
                const trainButton = document.getElementById('train-button');
                const stopButton = document.getElementById('stop-button');

                switch (data.command) {
                    case 'INIT_READY':
                        state.isWorkerReady = true;
                        setWorkerStatus('Ready', 'Worker is ready for commands.');
                        break;
                        
                    case 'CHECKPOINT_FOUND':
                        this.epochsTrained = data.metadata.epoch;
                        state.isCheckpointLoaded = true;
                        setWorkerStatus('Loaded', `Checkpoint found (Epoch: ${data.metadata.epoch}, Loss: ${data.metadata.loss.toFixed(4)}).`);
                        this.updateUI(true);
                        showAlert(`A model checkpoint was found! Trained up to Epoch ${data.metadata.epoch}.`, 'Checkpoint Found');
                        break;
                        
                    case 'CHECKPOINT_NOT_FOUND':
                        this.epochsTrained = 0;
                        state.isCheckpointLoaded = false;
                        setWorkerStatus('Ready', 'No checkpoint found. Starting fresh.');
                        this.updateUI(false);
                        break;
                        
                    case 'CHECKPOINT_DELETED':
                        this.epochsTrained = 0;
                        state.isCheckpointLoaded = false;
                        setWorkerStatus('Ready', 'Checkpoint deleted.');
                        this.updateUI(false);
                        showAlert('Model checkpoint has been deleted.', 'Checkpoint Deleted');
                        break;

                    case 'BATCH_END':
                        // data.batch, data.logs.loss
                        setWorkerStatus('Training', `Batch ${data.batch} Loss: ${data.logs.loss.toFixed(4)}`);
                        updateLossChart(`Batch ${data.batch}`, data.logs.loss, false);
                        break;

                    case 'EPOCH_END':
                        // data.epoch, data.logs.loss
                        this.epochsTrained = data.epoch + 1;
                        setWorkerStatus('Training', `Epoch ${data.epoch + 1} Loss: ${data.logs.loss.toFixed(4)}`);
                        updateLossChart(`Epoch ${data.epoch + 1}`, data.logs.loss, true);
                        state.isCheckpointLoaded = true; // Training creates a checkpoint
                        this.updateUI(true);
                        break;

                    case 'COMPLETE':
                        this.isTraining = false;
                        setWorkerStatus('Trained', 'Training completed successfully!');
                        this.updateUI(true);
                        showAlert('Mini Transformer finished training.', 'Training Complete', 5000);
                        break;

                    case 'ERROR':
                        this.isTraining = false;
                        setWorkerStatus('Error', data.message);
                        this.updateUI(state.isCheckpointLoaded); // Keep UI active if a checkpoint exists
                        showAlert(data.message, 'Training Error');
                        break;
                        
                    // Generation messages
                    case 'TOKEN_GENERATED':
                         // This is for streaming output during generation (optional for prototype)
                         break;
                    case 'GENERATION_COMPLETE':
                         // Handled by the await/promise in the main thread
                         break;
                }
            }

            /**
             * Checks IndexedDB for an existing model checkpoint via the worker.
             */
            checkCheckpoint() {
                 setWorkerStatus('Loading', 'Checking for existing checkpoint...');
                 this.worker.postMessage({ 
                    command: 'CHECK_CHECKPOINT', 
                    config: CONFIG 
                 });
            }

            /**
             * Starts the training process via the Web Worker.
             */
            startTraining(epochs, batchSize) {
                if (!dataHandler.getTokens().length || dataHandler.getVocabSize() < 5) {
                    showAlert('Please tokenize your training data first, and ensure the vocabulary size is adequate (>4).', 'Data Required');
                    return;
                }
                if (this.isTraining) return;

                this.isTraining = true;
                this.updateUI(true); // Disable train/delete, enable stop

                const { inputSequences, targetTokens } = dataHandler.createTrainingSequences(CONFIG.SEQUENCE_LENGTH);
                
                // Set the current training config in the global CONFIG
                CONFIG.train.epochs = epochs;
                CONFIG.train.batchSize = batchSize;

                // Reset chart on new training session (unless resuming)
                if (this.epochsTrained === 0) {
                     resetLossChart();
                }

                setWorkerStatus('Training', `Starting training from Epoch ${this.epochsTrained} for ${epochs} total epochs...`);
                
                // Send all necessary data to the worker
                this.worker.postMessage({
                    command: 'START',
                    config: CONFIG,
                    vocabSize: dataHandler.getVocabSize(),
                    tokenizerState: {
                        vocab: Array.from(dataHandler.vocab.entries()),
                        idToToken: Array.from(dataHandler.idToToken.entries())
                    },
                    trainingData: {
                        inputSequences: inputSequences,
                        targetTokens: targetTokens
                    }
                });
            }

            /**
             * Stops the training process via the Web Worker.
             */
            stopTraining() {
                if (!this.isTraining) return;
                
                this.worker.postMessage({ command: 'STOP' });
                this.isTraining = false;
                setWorkerStatus('Paused', 'Training manually stopped. Checkpoint saved.');
                this.updateUI(true);
            }
            
            /**
             * Deletes the model checkpoint from IndexedDB via the worker.
             */
            deleteCheckpoint() {
                 if (this.isTraining) {
                     showAlert('Stop training before deleting the checkpoint.', 'Action Blocked');
                     return;
                 }
                 this.worker.postMessage({ 
                    command: 'DELETE_CHECKPOINT', 
                    taskId: this.taskId,
                    config: CONFIG // Still send config in case worker needs DB name
                 });
            }

            /**
             * Generates text using the model (interfacing with the worker for model access/generation).
             * This function returns a Promise that resolves with the final generated text.
             * @param {string} prompt - The seed text.
             */
            generate(prompt) {
                if (this.isTraining) {
                    throw new Error("Cannot generate while the model is currently training.");
                }
                
                if (!state.isCheckpointLoaded) {
                     throw new Error("No trained model found. Please train the Transformer model first.");
                }

                return new Promise((resolve, reject) => {
                    // Tokenize the prompt
                    const promptTokens = prompt.toLowerCase().trim().split(/\s+/).filter(t => t.length > 0);
                    const inputTokens = promptTokens.map(token => dataHandler.vocab.get(token) || CONFIG.UNK_TOKEN_ID);
                    
                    if (inputTokens.length === 0) {
                        reject(new Error("Prompt contains no recognizable tokens."));
                        return;
                    }

                    // Reset the generation buffer in the main thread
                    let generatedTextBuffer = [...promptTokens];
                    
                    const generationListener = (event) => {
                        const data = event.data;
                        switch (data.command) {
                            case 'TOKEN_GENERATED':
                                generatedTextBuffer.push(data.token);
                                break;
                            case 'GENERATION_COMPLETE':
                                // Clean up listener
                                this.worker.removeEventListener('message', generationListener);
                                resolve(generatedTextBuffer.join(' '));
                                break;
                            case 'ERROR':
                                this.worker.removeEventListener('message', generationListener);
                                reject(new Error(data.message));
                                break;
                        }
                    };

                    // Add a temporary listener for generation
                    this.worker.addEventListener('message', generationListener);
                    
                    // Send generation command to the worker
                    this.worker.postMessage({
                        command: 'LOAD_MODEL_AND_GENERATE',
                        config: CONFIG,
                        vocabSize: dataHandler.getVocabSize(),
                        inputTokens: inputTokens,
                        sequenceLength: CONFIG.SEQUENCE_LENGTH,
                        maxNewTokens: CONFIG.MAX_NEW_TOKENS,
                        temperature: CONFIG.temperature,
                        // Passing maps as arrays for transfer
                        tokenToId: Array.from(dataHandler.vocab.entries()),
                        idToToken: Array.from(dataHandler.idToToken.entries()),
                        UNK_TOKEN_ID: CONFIG.UNK_TOKEN_ID
                    });
                });
            }

            /**
             * Updates the state of the buttons in the UI.
             * @param {boolean} checkpointExists - True if a checkpoint is loaded or training.
             */
            updateUI(checkpointExists) {
                const trainButton = document.getElementById('train-button');
                const stopButton = document.getElementById('stop-button');
                const deleteButton = document.getElementById('delete-checkpoint-button');
                
                if (this.isTraining) {
                    trainButton.disabled = true;
                    deleteButton.disabled = true;
                    stopButton.disabled = false;
                } else {
                    trainButton.disabled = false;
                    stopButton.disabled = true;
                    // Only allow delete if a checkpoint exists
                    deleteButton.disabled = !checkpointExists; 
                }
            }
        }


        // Main Application Functions
        // ==========================================================
        
        /**
         * Handles the main text generation request.
         */
        async function handleGeneration() {
            const prompt = document.getElementById('prompt-input').value.trim();
            const outputElement = document.getElementById('output-area');

            if (!prompt) {
                outputElement.innerHTML = '<em style="color: var(--google-red);">⚠ Please enter a prompt.</em>';
                showAlert('Please enter a prompt in the input field above.', 'Prompt Required');
                return;
            }

            // Check for required models based on mode
            if (state.mode === 'TRANSFORMER') {
                 if (!state.isCheckpointLoaded) {
                     outputElement.innerHTML = '<em style="color: var(--google-red);">⚠ No trained model found. Please train the Transformer model first.</em>';
                     showAlert('Please train the Transformer model before generating text.', 'Model Not Trained');
                     return;
                 }
                 if (!dataHandler.getVocabSize()) {
                     outputElement.innerHTML = '<em style="color: var(--google-red);">⚠ Data not tokenized. Please tokenize the training data.</em>';
                     showAlert('Please tokenize the data before generation.', 'Data Required');
                     return;
                 }
            } else if (state.mode === 'MARKOV' && !markovModel.transitionMatrix) {
                outputElement.innerHTML = '<em style="color: var(--google-red);">⚠ Markov Model is not trained. Please train the model.</em>';
                showAlert('Please train the Markov Model before generating text.', 'Model Not Trained');
                return;
            }

            outputElement.innerHTML = '<div style="text-align: center; padding: 20px;"><i class="fas fa-spinner fa-spin" style="font-size: 2em; color: var(--primary-blue);"></i><br><br><em>Generating text...</em></div>';

            try {
                let generatedText = "";
                const startTime = performance.now();

                if (state.mode === 'MARKOV') {
                    generatedText = markovModel.generate(prompt, CONFIG.MAX_NEW_TOKENS);
                } else {
                    // Transformer generation interfaces with the worker
                    generatedText = await miniTransformer.generate(prompt);
                }

                const totalTime = ((performance.now() - startTime) / 1000).toFixed(2);

                outputElement.innerHTML = `
                    <div style="background-color: #E8F0FE; padding: 12px; border-radius: 6px; margin-bottom: 15px; border-left: 4px solid var(--primary-blue);">
                        <strong style="color: var(--primary-blue);">Prompt:</strong> ${prompt}<br>
                        <small style="color: var(--secondary-text);">Mode: ${state.mode} | Generation Time: ${totalTime}s | Temperature: ${CONFIG.temperature}</small>
                    </div>
                    <div style="line-height: 1.8; color: var(--text-color); white-space: pre-wrap;">${generatedText}</div>
                `;
            } catch (error) {
                outputElement.innerHTML = `<em style="color: var(--google-red);">⚠ Error generating text: ${error.message}</em>`;
                showAlert(`Generation failed: ${error.message}`, 'Generation Error');
            }
        }
        
        /**
         * Handles the file upload process (mocked for this prototype).
         */
        function handleDataUpload() {
            // For a real app, this would use an <input type="file"> element.
            // Mock data for prototype simplicity.
             const mockData = "The quick brown fox jumps over the lazy dog. A lazy dog barks loudly at the quick brown fox. This is a very small corpus for training, so the resulting text will be simple and repetitive. The fox is fast. The dog is slow.";
             document.getElementById('training-data').value = mockData;
             dataHandler.tokenizeData();
             showAlert('Mock data loaded and tokenized automatically.', 'Data Loaded');
        }


        // Initialization
        // ==========================================================

        document.addEventListener('DOMContentLoaded', () => {
            // Initialize global instances
            dataHandler = new DataHandler(CONFIG.UNK_TOKEN, CONFIG.UNK_TOKEN_ID);
            markovModel = new MarkovModel();
            
            // FIX 3: Instantiate miniTransformer after the DOM is ready and set it globally
            miniTransformer = new MiniTransformer(CONFIG.taskId, CONFIG);

            // Initialize UI elements
            initializeChart();
            setMode(state.mode); // Set initial UI mode
            updateTemperature(CONFIG.temperature); // Set initial temperature UI
            
            // Auto-load mock data and tokenize
            handleDataUpload();
            
            // Set up event listeners that weren't inline
            document.getElementById('epochs').value = CONFIG.train.epochs;
            document.getElementById('batch-size').value = CONFIG.train.batchSize;
        });

    </script>
</body>
</html>

