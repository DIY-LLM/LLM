<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>DIY LLM Toolkit â€” Single-file (Full AWS + Transformer Encoder)</title>

  <!-- Fonts & icons -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

  <!-- External libraries (CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@3.9.1/dist/chart.min.js"></script>

  <style>
    :root{--primary-blue:#1A73E8;--surface-color:#fff;--bg:#f8f9fa;--text:#202124}
    body{font-family:Roboto, sans-serif;margin:0;background:var(--bg);color:var(--text)}
    .top-header{background:#12233b;color:#fff;padding:10px 20px;display:flex;align-items:center}
    .top-header h1{font-weight:400;margin:0 12px}
    .main-content-area{display:flex;min-height:calc(100vh - 60px)}
    .side-nav{width:240px;background:var(--surface-color);border-right:1px solid #e6e6e6;padding:12px}
    .nav-item{padding:10px 12px;color:#666;cursor:pointer}
    .nav-item.active{background:#eef6ff;border-left:4px solid var(--primary-blue);color:var(--primary-blue)}
    .content-wrapper{flex:1;padding:24px;max-width:1100px;margin:0 auto}
    h2{margin:0 0 12px 0}
    .mode-toggle{display:flex;justify-content:space-between;align-items:center;padding:10px;background:var(--surface-color);border-radius:8px;margin-bottom:16px;border-left:4px solid var(--primary-blue)}
    .upload-container{border:2px dashed #ddd;background:var(--surface-color);padding:20px;border-radius:8px;text-align:center;cursor:pointer}
    .data-source-card{display:inline-block;width:200px;padding:12px;background:var(--surface-color);border-radius:8px;margin:8px;text-align:center}
    #model-controls-panel{display:grid;grid-template-columns:repeat(auto-fit,minmax(200px,1fr));gap:12px;padding:12px;background:var(--surface-color);border-radius:8px}
    button{background:var(--primary-blue);color:#fff;border:none;padding:8px 12px;border-radius:6px;cursor:pointer}
    #metric-panel{background:var(--surface-color);padding:12px;border-radius:8px}
    canvas{background:#fff;border-radius:8px}
    .aws-config-modal{position:fixed;inset:0;display:none;align-items:center;justify-content:center;background:rgba(0,0,0,0.5);z-index:999}
    .aws-config-content{background:#fff;padding:18px;border-radius:8px;width:90%;max-width:720px}
    .training-status{padding:8px;background:#eaf2ff;border-left:4px solid var(--primary-blue);border-radius:6px;display:none}
    .training-status.active{display:block}
    .small-muted{font-size:12px;color:#666}
    input, textarea, select {border:1px solid #e0e0e0;padding:8px;border-radius:6px}
    .flex-row {display:flex;gap:8px;align-items:center}
    .pill {padding:4px 8px;border-radius:999px;background:#eef5ff;color:var(--primary-blue);font-weight:600}
    .footer {padding:12px;text-align:center;color:#666;font-size:13px}
    pre {white-space:pre-wrap;word-break:break-word}
    .kbd {background:#eee;border-radius:4px;padding:2px 6px;font-family:monospace}
  </style>
</head>
<body>
  <div class="top-header">
    <i class="fas fa-brain" style="font-size:20px;margin-right:8px"></i>
    <h1>DIY LLM Toolkit</h1>
    <div style="margin-left:auto;display:flex;gap:12px;align-items:center">
      <i class="fas fa-question-circle" title="Help"></i>
      <i class="fas fa-bell" title="Notifications"></i>
      <div style="width:34px;height:34px;background:#bcdfff;border-radius:50%"></div>
    </div>
  </div>

  <div class="main-content-area">
    <div class="side-nav">
      <div class="nav-item">Overview</div>
      <div class="nav-item active">Data & Tokenization</div>
      <div class="nav-item">Analytics & Metrics</div>
      <div class="nav-item">Model Training</div>
      <div class="nav-item">Text Generation</div>
      <hr>
      <div style="padding:8px">
        <div style="margin-bottom:8px"><button onclick="downloadCheckNoAwsKeys()">Download check_no_aws_keys.sh</button></div>
        <div style="font-size:12px;color:#666">No long-lived keys are stored in this file. Use temporary credentials.</div>
      </div>
    </div>

    <div class="content-wrapper">
      <div class="mode-toggle">
        <div id="current-mode-label">Current Mode: <strong id="model-mode-name">Markov Chain (Fallback)</strong><br><span class="small-muted">Switch to Transformer for encoder-based generation</span></div>
        <label style="display:flex;align-items:center;gap:8px">
          <span style="font-weight:600">Transformer</span>
          <input id="model-mode-toggle" type="checkbox" onchange="toggleModelMode(this.checked)">
        </label>
      </div>

      <section id="data-ingestion-section">
        <h2>Data Ingestion & Tokenization</h2>

        <div>
          <div class="data-source-card">
            <h3>Local Files</h3>
            <i class="fas fa-upload" style="font-size:28px;color:var(--primary-blue)"></i>
            <p class="small-muted">Drag, drop, or select multiple local files.</p>
            <label class="upload-label" for="file-input"><button style="background:#5f6368">Select Local Files</button></label>
          </div>

          <div class="data-source-card" onclick="showAwsConfigModal()">
            <h3>AWS S3 / SageMaker</h3>
            <i class="fab fa-aws" style="color:#FF9900;font-size:28px"></i>
            <p class="small-muted">Connect temporarily to AWS to start remote Transformer training and receive metrics.</p>
            <button style="background:#FF9900" onclick="event.stopPropagation(); showAwsConfigModal()">Connect AWS</button>
          </div>

          <div class="data-source-card">
            <h3>Sample Dataset</h3>
            <i class="fas fa-book" style="color:#9334E9;font-size:28px"></i>
            <p class="small-muted">Quickly load a small sample corpus.</p>
            <button style="background:#9334E9" onclick="loadSampleData()">Load Sample</button>
          </div>
        </div>

        <div id="drop-zone" class="upload-container" style="margin-top:12px">
          <i class="fas fa-file-import" style="font-size:36px;color:var(--primary-blue)"></i>
          <p class="small-muted">Drop files here or paste text into the textarea.</p>
          <input id="file-input" type="file" multiple accept=".txt,.md,.csv" style="display:block;margin:12px auto">
        </div>

        <div id="file-analytics-card" style="margin-top:12px;padding:12px;background:var(--surface-color);border-radius:8px">
          <h3>Uploaded Files Overview</h3>
          <div id="file-analytics-summary">0 Files Uploaded (0.0 MB)</div>
          <div id="progress-container" style="display:none;margin-top:8px">
            <div id="progress-bar" style="width:0%;height:20px;background:var(--primary-blue);color:#fff;text-align:center;border-radius:4px">0%</div>
          </div>
          <div id="file-preview" style="margin-top:12px;display:flex;flex-wrap:wrap;gap:8px"></div>
          <div style="margin-top:12px">
            <button onclick="tokenizer.exportVocabulary()" style="background:#5f6368">Export Vocab</button>
          </div>
        </div>

        <textarea id="text-input" placeholder="... or paste your text corpus directly here." style="width:100%;height:140px;margin-top:12px;border-radius:8px;padding:12px"></textarea>

        <div style="display:flex;gap:12px;margin-top:12px;flex-wrap:wrap">
          <button onclick="dataHandler.tokenizeAndBuildMatrix()">Tokenize & Preprocess</button>
          <button id="train-button" onclick="miniTransformer.trainOnText()" style="background:#3C769D"><i class="fas fa-bolt"></i> Train Model (Local)</button>
          <button onclick="startCloudTraining()" id="start-cloud-btn" style="display:none;background:#FF9900">Start Cloud Training</button>
        </div>

        <div class="training-status" id="training-status" style="margin-top:12px">
          <i class="fas fa-spinner fa-spin"></i> Training in progress... <span id="training-progress-text">Epoch 0/0</span>
        </div>
      </section>

      <hr style="margin:18px 0">

      <section id="model-section">
        <h2>Model Configuration & Execution</h2>
        <div id="model-controls-panel" style="display:none">
          <div>
            <label>Max Tokens (Inference)</label>
            <input id="max-tokens-input" type="number" value="50" min="10">
          </div>
          <div>
            <label>Temperature</label>
            <input id="temperature-input" type="number" value="0.7" step="0.1" min="0.1" max="2.0">
          </div>
          <div>
            <label>Embedding Dim (d_model)</label>
            <input id="embedding-size-input" type="number" value="128" min="16">
          </div>
          <div>
            <label>Number of Heads</label>
            <input id="num-heads-input" type="number" value="4" min="1" max="8">
          </div>
          <div>
            <label>Epochs</label>
            <input id="epochs-input" type="number" value="6" min="1">
          </div>
          <div>
            <label>Batch Size</label>
            <input id="batch-size-input" type="number" value="8" min="1">
          </div>
          <div>
            <label>Learning Rate</label>
            <input id="learning-rate-input" type="number" value="0.0005" step="0.0001" min="0.0001">
          </div>
        </div>

        <div id="model-dashboard" style="display:none;margin-top:12px;display:grid;grid-template-columns:2fr 1fr;gap:12px">
          <div id="chart-container" style="background:var(--surface-color);padding:12px;border-radius:8px">
            <h3><i class="fas fa-chart-area"></i> Live Loss Curve</h3>
            <canvas id="lossChart" height="180"></canvas>
          </div>
          <div id="metrics-panel" style="background:var(--surface-color);padding:12px;border-radius:8px">
            <h3><i class="fas fa-tachometer-alt"></i> Model Metrics</h3>
            <div>Current Loss: <strong id="metric-loss">N/A</strong></div>
            <div>Epochs Run: <strong id="metric-epochs">0</strong></div>
            <div>Vocab Size: <strong id="metric-vocab-size">0</strong></div>
            <div>Inference Latency: <strong id="metric-latency">N/A</strong></div>
            <div>Memory Footprint: <strong id="metric-memory">0 MB</strong></div>
            <div style="margin-top:12px;display:flex;flex-direction:column;gap:8px">
              <button onclick="miniTransformer.saveModel()">Save Model Weights</button>
              <button onclick="miniTransformer.loadModel()" style="background:#5f6368">Load Model Weights</button>
            </div>
          </div>
        </div>
      </section>

      <hr style="margin:18px 0">

      <section id="generation-section">
        <h2>Text Generation</h2>
        <div style="display:flex;gap:12px;margin-bottom:12px">
          <input id="prompt-input" type="text" placeholder="Enter your prompt..." style="flex:1;padding:10px;border-radius:6px;border:1px solid #ddd">
          <button onclick="generateTextFromModel()">Generate Text</button>
        </div>
        <div id="llm-output" style="padding:12px;background:#fff;border-radius:8px;min-height:100px">Generated text will appear here.</div>
      </section>

      <div style="margin-top:24px;background:var(--surface-color);padding:12px;border-radius:8px">
        <h3>Developer Panel & Tests</h3>
        <div style="display:flex;gap:8px;flex-wrap:wrap">
          <button onclick="openTestHarness()">Open Test Harness</button>
          <button onclick="emitMockMetrics()">Emit Mock Metrics</button>
          <button onclick="toggleCloudMode()">Toggle USE_CLOUD_TRAINING</button>
          <div id="dev-info" style="margin-left:auto">USE_CLOUD_TRAINING: <span id="cloud-flag">false</span></div>
        </div>
        <div style="margin-top:12px">
          <pre id="dev-log" style="max-height:240px;overflow:auto"></pre>
        </div>
      </div>

      <div class="footer">
        <strong>Security note:</strong> This single-file app intentionally contains no long-lived AWS credentials. Use Cognito, STS temporary credentials, or paste short-lived credentials only at runtime in the AWS modal. Placeholders: <code>S3_BUCKET_NAME</code>, <code>SAGEMAKER_ROLE_ARN</code>, <code>SAGEMAKER_IMAGE_URI</code>, <code>API_GATEWAY_WS_URL</code>.
      </div>

    </div>
  </div>

  <!-- AWS Config Modal -->
  <div id="aws-config-modal" class="aws-config-modal">
    <div class="aws-config-content">
      <h3><i class="fab fa-aws"></i> AWS Configuration (temporary)</h3>
      <p class="small-muted">Provide temporary credentials (Cognito / STS / session tokens) or a metrics WebSocket URL. Do NOT store long-lived keys here.</p>

      <div style="display:grid;grid-template-columns:1fr 1fr;gap:8px">
        <div>
          <label>AWS Region</label>
          <input id="aws-region" value="us-east-1" />
        </div>
        <div>
          <label>S3 Bucket</label>
          <input id="aws-bucket" placeholder="S3_BUCKET_NAME" />
        </div>
      </div>

      <div style="display:grid;grid-template-columns:1fr;gap:8px;margin-top:8px">
        <label>Access Key ID (temporary)</label>
        <input id="aws-access-key" placeholder="ASIA..." />
        <label>Secret Access Key (temporary)</label>
        <input id="aws-secret-key" type="password" placeholder="wJalr..." />
        <label>Session Token (optional)</label>
        <input id="aws-session-token" />
        <label>Session Role ARN (optional override)</label>
        <input id="aws-role-arn" placeholder="SAGEMAKER_ROLE_ARN" />
        <label>Metrics WebSocket URL (wss://...)</label>
        <input id="aws-ws-url" placeholder="wss://API_GATEWAY_WS_URL" />
        <label>SageMaker Training Image URI (ECR)</label>
        <input id="aws-image-uri" placeholder="SAGEMAKER_TRAINING_IMAGE_URI" />
        <label>Default Instance Type</label>
        <input id="aws-instance-type" placeholder="ml.g4dn.xlarge" value="ml.g4dn.xlarge" />
      </div>

      <div style="display:flex;justify-content:flex-end;gap:8px;margin-top:12px">
        <button onclick="closeAwsConfigModal()" style="background:#5f6368">Cancel</button>
        <button onclick="connectAwsFromModal()" style="background:#FF9900">Connect (temporary)</button>
      </div>
    </div>
  </div>

  <!-- Test harness modal (inline) -->
  <div id="test-harness-modal" class="aws-config-modal" style="display:none">
    <div class="aws-config-content">
      <h3>Test Harness</h3>
      <p class="small-muted">Run quick integration tests locally and with mocked AWS flows.</p>
      <div style="display:flex;gap:8px">
        <button id="run-local-test">Run Local Training Test</button>
        <button id="run-cloud-mock-test">Run Mock Cloud Offload Test</button>
        <button id="close-test">Close</button>
      </div>
      <pre id="test-log" style="max-height:360px;overflow:auto;margin-top:8px"></pre>
    </div>
  </div>

  <script type="module">
/*
  FULL-FEATURED single-file app (Functional Realism)
  - AWS handling: dynamic AWS SDK v3 imports from CDN (option A)
  - Transformer: true multi-head self-attention encoder implemented in TF.js (option a)
  - All UI, training loops, and AWS orchestration live in this file
  - Placeholders ALL_CAPS for S3 bucket, role ARNs, image URIs, WS URL
  - No long-lived AWS keys stored
*/

/* ======================
   Configuration placeholders
   ====================== */
const USE_CLOUD_TRAINING_DEFAULT = false; // default; toggleable in dev panel
const S3_BUCKET_PLACEHOLDER = "S3_BUCKET_NAME";
const SAGEMAKER_ROLE_ARN_PLACEHOLDER = "SAGEMAKER_ROLE_ARN";
const SAGEMAKER_IMAGE_URI_PLACEHOLDER = "SAGEMAKER_TRAINING_IMAGE_URI";
const API_GATEWAY_WS_URL_PLACEHOLDER = "wss://API_GATEWAY_WS_URL";
const DEFAULT_INSTANCE_TYPE_PLACEHOLDER = "ml.g4dn.xlarge";

/* ======================
   Runtime AWS Config (ephemeral; editable in modal)
   ====================== */
const AwsRuntimeConfig = {
  region: 'us-east-1',
  accessKeyId: null,
  secretAccessKey: null,
  sessionToken: null,
  bucket: S3_BUCKET_PLACEHOLDER,
  wsUrl: API_GATEWAY_WS_URL_PLACEHOLDER,
  roleArn: SAGEMAKER_ROLE_ARN_PLACEHOLDER,
  sagemakerImageUri: SAGEMAKER_IMAGE_URI_PLACEHOLDER,
  instanceType: DEFAULT_INSTANCE_TYPE_PLACEHOLDER,
  projectName: 'DIY_LLM_TOOLKIT'
};

/* ======================
   DEV LOGGING helper
   ====================== */
function logDev(...args) {
  const el = document.getElementById('dev-log');
  if (el) { el.textContent += args.map(a => (typeof a === 'object' ? JSON.stringify(a,null,2) : String(a))).join(' ') + '\n'; el.scrollTop = el.scrollHeight; }
  console.debug(...args);
}

/* ======================
   Metrics WebSocket bus
   ====================== */
const AwsMetrics = (function(){
  let ws = null;
  const listeners = new Set();

  function connect(url = null) {
    const wsUrl = url || AwsRuntimeConfig.wsUrl || API_GATEWAY_WS_URL_PLACEHOLDER;
    if (!wsUrl || wsUrl.includes('API_GATEWAY_WS_URL')) {
      logDev('No valid metrics WebSocket URL provided; metrics disabled.');
      return;
    }
    if (ws) { ws.close(); ws = null; }
    ws = new WebSocket(wsUrl);
    ws.onopen = () => {
      dispatch({ type: 'status', state: 'CONNECTED', message: 'WS open', timestamp: new Date().toISOString() });
      logDev('Metrics WS connected:', wsUrl);
    };
    ws.onmessage = (evt) => {
      try {
        const payload = JSON.parse(evt.data);
        dispatch(payload);
      } catch (err) {
        logDev('Invalid WS message', evt.data);
      }
    };
    ws.onclose = () => { dispatch({ type:'status', state:'DISCONNECTED', message:'WS closed', timestamp:new Date().toISOString() }); ws = null; };
    ws.onerror = (e) => { dispatch({ type:'status', state:'FAILED', message:'WS error', timestamp:new Date().toISOString() }); console.error(e); };
  }

  function disconnect() { if (ws) ws.close(); ws = null; }

  function onMetric(fn) { listeners.add(fn); return () => listeners.delete(fn); }

  function dispatch(obj) { listeners.forEach(fn => { try { fn(obj); } catch(e) { console.error('metric listener error', e); } }); }

  function appendMetric(obj) { dispatch(obj); }

  return { connect, disconnect, onMetric, appendMetric };
})();

/* ======================
   AWS SageMaker orchestration via SDK v3 (dynamic ESM import)
   ====================== */
const AwsTraining = (function(){
  let sagemakerClient = null;
  let s3Client = null;
  let awsLibs = null;
  let initialized = false;

  async function ensureClients() {
    if (initialized && sagemakerClient && s3Client) return awsLibs;
    try {
      // import the modular clients
      const [sagemakerModule, s3Module] = await Promise.all([
        import('https://cdn.jsdelivr.net/npm/@aws-sdk/client-sagemaker@3.375.0/dist-es/index.js'),
        import('https://cdn.jsdelivr.net/npm/@aws-sdk/client-s3@3.375.0/dist-es/index.js')
      ]);
      awsLibs = { ...sagemakerModule, ...s3Module };
      const creds = AwsRuntimeConfig.accessKeyId ? {
        credentials: {
          accessKeyId: AwsRuntimeConfig.accessKeyId,
          secretAccessKey: AwsRuntimeConfig.secretAccessKey,
          sessionToken: AwsRuntimeConfig.sessionToken || undefined
        }
      } : {};
      sagemakerClient = new awsLibs.SageMakerClient({ region: AwsRuntimeConfig.region, ...creds });
      s3Client = new awsLibs.S3Client({ region: AwsRuntimeConfig.region, ...creds });
      initialized = true;
      logDev('AWS SDK loaded and clients initialized.');
      return awsLibs;
    } catch (err) {
      console.error('Failed to load AWS SDK v3:', err);
      throw new Error('Unable to load AWS SDK v3 in browser. Ensure network connectivity to CDN.');
    }
  }

  async function startSageMakerTraining(runId, trainConfig = {}) {
    if (!window.USE_CLOUD_TRAINING) throw new Error('Cloud training disabled (USE_CLOUD_TRAINING=false).');
    const aws = await ensureClients();
    const jobName = `diy-llm-${runId}-${Date.now()}`.replace(/[^a-zA-Z0-9-]/g,'').toLowerCase();
    const inputS3 = trainConfig.inputS3Uri || `s3://${AwsRuntimeConfig.bucket}/${AwsRuntimeConfig.projectName}/${runId}/training/`;
    const outputS3 = trainConfig.outputS3Uri || `s3://${AwsRuntimeConfig.bucket}/${AwsRuntimeConfig.projectName}/${runId}/output/`;
    const checkpointS3Uri = `s3://${AwsRuntimeConfig.bucket}/${AwsRuntimeConfig.projectName}/${runId}/checkpoints/`;
    const hparams = Object.assign({
      epochs: String(trainConfig.epochs || 3),
      batch_size: String(trainConfig.batchSize || 16),
      learning_rate: String(trainConfig.learningRate || 0.0005),
      seq_len: String(trainConfig.seqLen || 128),
      vocab_size: String(trainConfig.vocabSize || 32000),
      quantize: trainConfig.quantize ? 'true' : 'false',
      checkpoint_s3_uri: checkpointS3Uri
    }, trainConfig.hyperParameters || {});

    const imageUri = AwsRuntimeConfig.sagemakerImageUri || SAGEMAKER_IMAGE_URI_PLACEHOLDER;
    if (!imageUri || imageUri.includes('SAGEMAKER_TRAINING_IMAGE_URI')) {
      throw new Error('SageMaker training image URI not set. Place SAGEMAKER_IMAGE_URI in modal before starting.');
    }

    const createTrainingJobParams = {
      TrainingJobName: jobName,
      AlgorithmSpecification: { TrainingImage: imageUri, TrainingInputMode: 'File' },
      RoleArn: AwsRuntimeConfig.roleArn || SAGEMAKER_ROLE_ARN_PLACEHOLDER,
      InputDataConfig: [
        { ChannelName:'training',
          DataSource: { S3DataSource: { S3DataType:'S3Prefix', S3Uri: inputS3, S3DataDistributionType:'FullyReplicated' } },
          ContentType: 'text/plain'
        }
      ],
      OutputDataConfig: { S3OutputPath: outputS3 },
      ResourceConfig: {
        InstanceCount: trainConfig.instanceCount || 1,
        InstanceType: trainConfig.instanceType || AwsRuntimeConfig.instanceType || DEFAULT_INSTANCE_TYPE_PLACEHOLDER,
        VolumeSizeInGB: 100
      },
      StoppingCondition: { MaxRuntimeInSeconds: 60 * 60 * 24 },
      HyperParameters: hparams
    };

    try {
      const { CreateTrainingJobCommand } = aws;
      const cmd = new CreateTrainingJobCommand(createTrainingJobParams);
      const res = await sagemakerClient.send(cmd);
      AwsMetrics.appendMetric({ type:'status', state:'STARTING', message:`Started job ${jobName}`, timestamp:new Date().toISOString() });
      logDev('SageMaker createTrainingJob response', res);
      return { trainingJobName: jobName, awsResponse: res };
    } catch (err) {
      console.error('CreateTrainingJob error', err);
      AwsMetrics.appendMetric({ type:'status', state:'FAILED', message: err.message || String(err), timestamp:new Date().toISOString() });
      throw err;
    }
  }

  async function describeTrainingJob(trainingJobName) {
    const aws = await ensureClients();
    const { DescribeTrainingJobCommand } = aws;
    try {
      const cmd = new DescribeTrainingJobCommand({ TrainingJobName: trainingJobName });
      const res = await sagemakerClient.send(cmd);
      return res;
    } catch (err) {
      console.error('DescribeTrainingJob error', err);
      throw err;
    }
  }

  async function uploadCheckpoint(blob, key) {
    const aws = await ensureClients();
    const { PutObjectCommand } = aws;
    const params = { Bucket: AwsRuntimeConfig.bucket || S3_BUCKET_PLACEHOLDER, Key: key, Body: blob };
    try {
      const cmd = new PutObjectCommand(params);
      const res = await s3Client.send(cmd);
      return res;
    } catch (err) {
      console.error('S3 PutObject error', err);
      throw err;
    }
  }

  return { ensureClients, startSageMakerTraining, describeTrainingJob, uploadCheckpoint };
})();

/* ======================
   Transformer encoder (multi-head attention) in TF.js
   Functional implementation focused on inference & small training loops in browser.
   This is a compact but correct Transformer encoder: token embedding, positional encoding,
   multi-head self-attention, feed-forward, layernorm, and stacking.
   ====================== */
function createPositionalEncoding(maxLen, dModel) {
  // returns a Tensor of shape [1, maxLen, dModel]
  const pe = tf.tidy(() => {
    const pos = tf.range(0, maxLen, 1, 'float32').expandDims(1); // [maxLen,1]
    const i = tf.range(0, dModel, 1, 'float32').expandDims(0); // [1,dModel]
    const angleRates = tf.div(tf.pow(10000, tf.div(tf.floorDiv(i, 2).toFloat(), dModel)), 1.0);
    // compute pos / (10000^(2i/dModel))
    const angle = tf.div(pos.toFloat(), angleRates);
    const sines = tf.sin(angle.slice([0,0],[maxLen, Math.floor(dModel/2)]));
    const cosines = tf.cos(angle.slice([0, Math.floor(dModel/2)],[maxLen, Math.ceil(dModel/2)]));
    const interleave = tf.concat([sines, cosines], 1);
    const peFinal = interleave.reshape([1, maxLen, dModel]);
    return peFinal;
  });
  return pe;
}

/* Multi-head attention layer implemented with basic dense layers */
class MultiHeadSelfAttention {
  constructor(dModel, numHeads) {
    if (dModel % numHeads !== 0) throw new Error('dModel must be divisible by numHeads');
    this.dModel = dModel;
    this.numHeads = numHeads;
    this.depth = dModel / numHeads;
    // weights (dense layers) will be created lazily inside build()
    this.built = false;
  }

  build(inputShape) {
    // inputShape: [batch, seqLen, dModel]
    const wInit = tf.initializers.glorotUniform({});
    this.wq = tf.variable(wInit.apply([this.dModel, this.dModel]));
    this.wk = tf.variable(wInit.apply([this.dModel, this.dModel]));
    this.wv = tf.variable(wInit.apply([this.dModel, this.dModel]));
    this.wo = tf.variable(wInit.apply([this.dModel, this.dModel]));
    this.built = true;
  }

  splitHeads(x, batchSize) {
    // x shape: [batch, seqLen, dModel] -> [batch, numHeads, seqLen, depth]
    return tf.tidy(() => {
      const reshaped = x.reshape([batchSize, -1, this.numHeads, this.depth]);
      return reshaped.transpose([0,2,1,3]);
    });
  }

  scaledDotProductAttention(q, k, v, mask=null) {
    // q,k,v: [batch, numHeads, seqLen, depth]
    return tf.tidy(() => {
      const matmulQK = tf.matMul(q, k, false, true); // [batch, numHeads, seqLen, seqLen]
      const dk = tf.scalar(this.depth, 'float32');
      const scaled = matmulQK.div(tf.sqrt(dk));
      let logits = scaled;
      if (mask) {
        // mask shape broadcastable to logits
        const bigNeg = tf.scalar(-1e9);
        logits = logits.add(mask.mul(bigNeg));
      }
      const attentionWeights = tf.softmax(logits, -1);
      const output = tf.matMul(attentionWeights, v); // [batch, numHeads, seqLen, depth]
      return output;
    });
  }

  apply(x, mask=null) {
    // x: tf.Tensor [batch, seqLen, dModel]
    return tf.tidy(() => {
      if (!this.built) this.build(x.shape);
      const batchSize = x.shape[0];
      const q = tf.matMul(x, this.wq); // [batch, seqLen, dModel]
      const k = tf.matMul(x, this.wk);
      const v = tf.matMul(x, this.wv);
      const qHeads = this.splitHeads(q, batchSize);
      const kHeads = this.splitHeads(k, batchSize);
      const vHeads = this.splitHeads(v, batchSize);
      const scaledAttn = this.scaledDotProductAttention(qHeads, kHeads, vHeads, mask); // [batch,numHeads,seqLen,depth]
      const transposed = scaledAttn.transpose([0,2,1,3]); // [batch,seqLen,numHeads,depth]
      const concat = transposed.reshape([batchSize, -1, this.dModel]); // [batch,seqLen,dModel]
      const output = tf.matMul(concat, this.wo); // [batch,seqLen,dModel]
      return output;
    });
  }

  // expose variables for optimizer updates
  getWeights() { return [this.wq, this.wk, this.wv, this.wo]; }
}

/* Positionwise Feed-Forward Network */
function pointWiseFeedForwardNetwork(dModel, dff) {
  const w1 = tf.variable(tf.initializers.glorotUniform({}).apply([dModel, dff]));
  const b1 = tf.variable(tf.zeros([dff]));
  const w2 = tf.variable(tf.initializers.glorotUniform({}).apply([dff, dModel]));
  const b2 = tf.variable(tf.zeros([dModel]));
  return function(x) {
    return tf.tidy(()=> {
      const r1 = tf.relu(tf.add(tf.matMul(x, w1), b1));
      return tf.add(tf.matMul(r1, w2), b2);
    });
  };
}

/* Layer Normalization (simple implementation) */
function layerNorm(x, epsilon=1e-6) {
  return tf.tidy(() => {
    const mean = tf.mean(x, -1, true);
    const variance = tf.mean(tf.square(tf.sub(x, mean)), -1, true);
    const normalized = tf.div(tf.sub(x, mean), tf.sqrt(tf.add(variance, epsilon)));
    // scale & shift can be applied via learned gamma & beta if desired â€” using identity for simplicity
    return normalized;
  });
}

/* Transformer Encoder Layer */
class EncoderLayer {
  constructor(dModel, numHeads, dff, dropoutRate=0.1) {
    this.mha = new MultiHeadSelfAttention(dModel, numHeads);
    this.ffn = pointWiseFeedForwardNetwork(dModel, dff);
    this.dropoutRate = dropoutRate;
    // learnable gammas/betas could be added for layernorm; omitted for brevity
  }

  apply(x, mask=null) {
    return tf.tidy(() => {
      // Self-attention
      const attnOutput = this.mha.apply(x, mask); // [batch, seqLen, dModel]
      const attnOutDrop = attnOutput; // dropout could be applied (tf.layers.dropout not used)
      const out1 = layerNorm(tf.add(x, attnOutDrop)); // Residual + norm

      // Feed-forward
      const ffnOutput = this.ffn(out1);
      const ffnOutDrop = ffnOutput;
      const out2 = layerNorm(tf.add(out1, ffnOutDrop));
      return out2;
    });
  }

  getWeights() {
    return [...this.mha.getWeights() /* plus ffn variables not tracked here */];
  }
}

/* Full Transformer Encoder stack */
class TransformerEncoder {
  constructor(numLayers, dModel, numHeads, dff, inputVocabSize, maximumPositionEncoding) {
    this.numLayers = numLayers;
    this.dModel = dModel;
    this.numHeads = numHeads;
    this.dff = dff;
    this.inputVocabSize = inputVocabSize;
    this.maximumPositionEncoding = maximumPositionEncoding;

    // Embedding matrix
    this.embedding = tf.variable(tf.initializers.randomNormal({ mean:0, stddev:0.1 }).apply([inputVocabSize, dModel]));
    this.posEncoding = createPositionalEncoding(maximumPositionEncoding, dModel); // [1, maxLen, dModel]
    // Encoder layers
    this.encLayers = [];
    for (let i=0;i<numLayers;i++) this.encLayers.push(new EncoderLayer(dModel, numHeads, dff));
  }

  embed(x) {
    // x: integer tensor shape [batch, seqLen]
    return tf.tidy(() => {
      const embedded = tf.gather(this.embedding, x); // [batch, seqLen, dModel]
      // scale embeddings by sqrt(dModel)
      const scaled = embedded.mul(tf.sqrt(tf.scalar(this.dModel)));
      // add positional encoding (slice to seqLen)
      const seqLen = x.shape[1];
      const posSlice = this.posEncoding.slice([0,0,0],[1,seqLen,this.dModel]);
      return scaled.add(posSlice);
    });
  }

  apply(xIds, mask=null) {
    // xIds: tf.Tensor int32 [batch, seqLen]
    return tf.tidy(() => {
      let x = this.embed(xIds);
      for (let i=0;i<this.numLayers;i++) {
        x = this.encLayers[i].apply(x, mask);
      }
      return x; // [batch, seqLen, dModel]
    });
  }

  getEmbeddingWeights() { return [this.embedding]; }
}

/* ======================
   Tokenizer & DataHandler (simple word tokenizer preserving public API)
   ====================== */
class Tokenizer {
  constructor() {
    this.vocab = {};
    this.idToToken = {};
    this.tokenCounts = {};
    this.nextId = 1; // reserve 0 for PAD
  }
  cleanText(text) {
    return text.toLowerCase().replace(/[.,\/#!$%\^&\*;:{}=\-_`~()"]/g,'').replace(/\s{2,}/g,' ').trim();
  }
  train(corpus) {
    const cleaned = this.cleanText(corpus);
    const words = cleaned.split(/\s+/).filter(Boolean);
    this.vocab = {}; this.idToToken = {}; this.tokenCounts = {}; this.nextId = 1;
    words.forEach(w=>{
      if (!this.vocab[w]) { this.vocab[w] = this.nextId; this.idToToken[this.nextId] = w; this.nextId++; }
      this.tokenCounts[w] = (this.tokenCounts[w]||0)+1;
    });
    this.idToToken[0] = '[PAD]';
    window.state.vocabSize = Object.keys(this.vocab).length;
    logDev('Tokenizer trained. Vocab size:', window.state.vocabSize);
  }
  encode(text, maxLen=null) {
    const cleaned = this.cleanText(text);
    const words = cleaned.split(/\s+/).filter(Boolean);
    const ids = words.map(w => this.vocab[w] || 0);
    if (maxLen) {
      if (ids.length > maxLen) return ids.slice(0,maxLen);
      while (ids.length < maxLen) ids.push(0);
    }
    return ids;
  }
  decode(ids) {
    return ids.map(id => this.idToToken[id] || '[UNK]').join(' ');
  }
  exportVocabulary() {
    const blob = new Blob([JSON.stringify(this.idToToken, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a'); a.href = url; a.download = 'llm_vocab.json'; document.body.appendChild(a); a.click(); a.remove();
    URL.revokeObjectURL(url);
    alert('Vocabulary exported as llm_vocab.json');
  }
}

class DataHandler {
  constructor(tokenizer) {
    this.tokenizer = tokenizer;
    this.corpusText = "";
    this.tokenIDs = [];
    this.uploadedFiles = [];
    this.totalSizeMB = 0;
    this.transitionMatrix = null;
    this.setupListeners();
  }
  setupListeners() {
    document.getElementById('file-input').addEventListener('change', (e)=>this.handleFiles(e.target.files));
    const drop = document.getElementById('drop-zone');
    ['dragenter','dragover','dragleave','drop'].forEach(ev => {
      drop.addEventListener(ev, (e)=>{ e.preventDefault(); e.stopPropagation(); });
    });
    drop.addEventListener('drop', (e)=> this.handleFiles(e.dataTransfer.files));
    document.getElementById('text-input').addEventListener('input', (e)=> this.corpusText = e.target.value);
  }
  addFileToPreview(f) {
    const preview = document.getElementById('file-preview');
    const el = document.createElement('div'); el.style.padding='6px 8px'; el.style.background='#f1f3f4'; el.style.borderRadius='12px';
    el.innerText = f.name || 'file';
    preview.appendChild(el);
    this.uploadedFiles.push(f);
    this.updateSummary();
  }
  updateSummary() {
    document.getElementById('file-analytics-summary').innerText = `${this.uploadedFiles.length} Files Uploaded (${this.totalSizeMB.toFixed(2)} MB)`;
  }
  async handleFiles(files) {
    if (!files || files.length===0) return;
    const progressBar = document.getElementById('progress-bar');
    progressBar.style.width = '0%'; document.getElementById('progress-container').style.display = 'block';
    this.corpusText=''; this.uploadedFiles=[]; this.totalSizeMB=0; document.getElementById('file-preview').innerHTML='';
    for (let i=0;i<files.length;i++) {
      const f = files[i];
      const reader = new FileReader();
      await new Promise((resolve)=> {
        reader.onload = (e)=> { this.corpusText += e.target.result + ' '; this.totalSizeMB += f.size/(1024*1024); this.addFileToPreview(f); resolve(); };
        reader.readAsText(f);
      });
      progressBar.style.width = `${Math.round(((i+1)/files.length)*100)}%`;
    }
    document.getElementById('text-input').value = this.corpusText.trim();
    document.getElementById('progress-container').style.display = 'none';
    alert(`Loaded ${files.length} files.`);
  }
  tokenizeAndBuildMatrix() {
    const corpus = (document.getElementById('text-input').value || this.corpusText || '').trim();
    if (!corpus) { alert('Please provide a text corpus first.'); return; }
    this.tokenizer.train(corpus);
    this.tokenIDs = this.tokenizer.encode(corpus);
    this.buildMarkovTransitionMatrix(this.tokenIDs);
    document.getElementById('metric-vocab-size').innerText = window.state.vocabSize.toLocaleString();
    document.getElementById('model-controls-panel').style.display = window.state.mode==='TRANSFORMER' ? 'grid' : 'none';
    document.getElementById('model-dashboard').style.display = window.state.mode==='TRANSFORMER' ? 'grid' : 'none';
    alert('Tokenization complete.');
  }
  buildMarkovTransitionMatrix(ids) {
    const size = window.state.vocabSize + 1;
    const matrix = Array(size).fill(0).map(()=>Array(size).fill(0));
    const rowSums = Array(size).fill(0);
    for (let i=0;i<ids.length-1;i++){
      const a = ids[i] < size ? ids[i] : 0;
      const b = ids[i+1] < size ? ids[i+1] : 0;
      matrix[a][b]++; rowSums[a]++;
    }
    for (let i=0;i<size;i++) if (rowSums[i]>0) for (let j=0;j<size;j++) matrix[i][j] /= rowSums[i];
    this.transitionMatrix = matrix;
  }
}

/* ======================
   Public instances & state
   ====================== */
window.USE_CLOUD_TRAINING = USE_CLOUD_TRAINING_DEFAULT;
window.state = { mode: 'MARKOV', vocabSize: 0, currentLoss: 'N/A', epochsRun: 0, latency: 'N/A', isTraining: false };

window.tokenizer = new Tokenizer();
window.dataHandler = new DataHandler(window.tokenizer);
window.markovChain = new MarkovChain ? new MarkovChain(window.tokenizer, window.dataHandler) : null;
/* Since MarkovChain defined later, create simple inline fallback if missing */
if (!window.markovChain) {
  class MarkovChainLocal { constructor(tokenizer, dataHandler){ this.tokenizer=tokenizer; this.dataHandler=dataHandler; } generate(prompt){ const matrix=this.dataHandler.transitionMatrix; if(!matrix) return 'No matrix'; const words=this.tokenizer.cleanText(prompt).split(/\s+/).filter(Boolean); if(!words.length) return ''; let cur=this.tokenizer.vocab[words[words.length-1]]||0; const out=this.tokenizer.encode(prompt); for(let i=0;i<50;i++){ const probs=matrix[cur]||[]; let r=Math.random(), s=0, next=0; for(let j=0;j<probs.length;j++){ s+=probs[j]; if(r<=s){ next=j; break; } } if(next===0) break; out.push(next); cur=next; } return this.tokenizer.decode(out); } }
  window.markovChain = new MarkovChainLocal(window.tokenizer, window.dataHandler);
}

window.tokenizer = window.tokenizer;
window.dataHandler = window.dataHandler;

/* ======================
   Full Transformer wrapper for local training & inference
   ====================== */
class MiniTransformer {
  constructor(tokenizer, dataHandler) {
    this.tokenizer = tokenizer;
    this.dataHandler = dataHandler;
    this.encoder = null;
    this.projectionW = null; // final linear to vocab
    this.built = false;
    this.lossHistory = [];
    this.isTraining = false;
  }

  buildModel(params) {
    const vocabSize = window.state.vocabSize + 1;
    const dModel = params.dModel || 128;
    const numHeads = params.numHeads || 4;
    const dff = params.dff || 512;
    const numLayers = params.numLayers || 2;
    const maxPos = params.maxPos || 128;

    this.encoder = new TransformerEncoder(numLayers, dModel, numHeads, dff, vocabSize, maxPos);
    // final projection: dModel -> vocabSize
    this.projectionW = tf.variable(tf.initializers.glorotUniform({}).apply([dModel, vocabSize]));
    this.built = true;
    logDev('Built Transformer encoder with', { numLayers, dModel, numHeads, dff, vocabSize, maxPos });
  }

  async trainOnText() {
    if (window.state.isTraining) return;
    if (!this.dataHandler || this.dataHandler.tokenIDs.length < 20) { alert('Need more corpus to train locally.'); return; }
    window.state.isTraining = true;
    document.getElementById('training-status').classList.add('active');
    initLossChart();

    // Build with UI params if not built
    const dModel = parseInt(document.getElementById('embedding-size-input').value || '128');
    const numHeads = parseInt(document.getElementById('num-heads-input').value || '4');
    const dff = Math.max(256, dModel*2);
    const numLayers = 2;
    const maxPos = 128;
    if (!this.built) this.buildModel({ dModel, numHeads, dff, numLayers, maxPos });

    // Prepare dataset (simple sliding window)
    const seqLen = Math.min(32, maxPos); // small sequence length for browser
    const tokens = this.dataHandler.tokenIDs;
    const xs = []; const ys = [];
    for (let i=0;i<tokens.length - seqLen;i+=1) {
      const inSeq = tokens.slice(i, i+seqLen);
      const outToken = tokens[i+seqLen];
      xs.push(inSeq);
      ys.push(outToken);
      if (xs.length > 2000) break; // limit for browser
    }
    if (xs.length === 0) { alert('Not enough sequences for training.'); window.state.isTraining=false; return; }

    // create tensors
    const xTensor = tf.tensor2d(xs, [xs.length, seqLen], 'int32'); // [N, seqLen]
    const yTensor = tf.tensor1d(ys, 'int32'); // [N]

    // Training parameters
    const epochs = parseInt(document.getElementById('epochs-input').value || '6');
    const batchSize = parseInt(document.getElementById('batch-size-input').value || '8');
    const learningRate = parseFloat(document.getElementById('learning-rate-input').value || '0.0005');
    const optimizer = tf.train.adam(learningRate);

    for (let epoch=0; epoch<epochs; epoch++) {
      if (!window.state.isTraining) break;
      // simple epoch loop with batches
      const numBatches = Math.ceil(xs.length / batchSize);
      let epochLoss = 0;
      for (let b=0;b<numBatches;b++) {
        const start = b * batchSize;
        const end = Math.min((b+1)*batchSize, xs.length);
        const xb = xTensor.slice([start,0],[end-start, seqLen]);
        const yb = yTensor.slice([start],[end-start]);
        const loss = await optimizer.minimize(() => {
          // forward
          const encOut = this.encoder.apply(xb); // [batch, seqLen, dModel]
          // take last token representation for prediction
          const last = encOut.slice([0, seqLen-1, 0], [end-start, 1, this.encoder.dModel]).reshape([end-start, this.encoder.dModel]);
          const logits = tf.matMul(last, this.projectionW); // [batch, vocabSize]
          const yOneHot = tf.oneHot(yb, window.state.vocabSize+1);
          const ce = tf.losses.softmaxCrossEntropy(yOneHot, logits).mean();
          return ce;
        }, true);
        const lossVal = loss.dataSync()[0];
        epochLoss += lossVal;
        // emit metric for chart (step = epoch * batches + b)
        const step = epoch * numBatches + b + 1;
        const avgLoss = epochLoss / (b+1);
        AwsMetrics.appendMetric({ type:'metrics', step, epoch: epoch+1, loss: avgLoss, lr: learningRate, timestamp: new Date().toISOString() });
        document.getElementById('training-progress-text').innerText = `Epoch ${epoch+1}/${epochs} Batch ${b+1}/${numBatches} - Loss: ${avgLoss.toFixed(4)}`;
        await tf.nextFrame();
      }
      this.lossHistory.push(epochLoss / numBatches);
      window.state.currentLoss = this.lossHistory[this.lossHistory.length-1];
      window.state.epochsRun = epoch+1;
      if (lossChart) { lossChart.data.labels.push(epoch+1); lossChart.data.datasets[0].data.push(window.state.currentLoss); lossChart.update('none'); }
    }

    xTensor.dispose(); yTensor.dispose();
    window.state.isTraining = false;
    document.getElementById('training-status').classList.remove('active');
    const mem = tf.memory();
    document.getElementById('metric-memory').innerText = `${(mem.numBytes/1024/1024).toFixed(2)} MB`;
    alert('Local Transformer training complete.');
  }

  async generate(prompt) {
    if (!this.built) return 'No local model built.';
    const start = performance.now();
    const seqLen = 32;
    let inputIds = this.tokenizer.encode(prompt, seqLen);
    if (inputIds.length < seqLen) {
      while (inputIds.length < seqLen) inputIds.unshift(0);
    }
    const maxTokens = parseInt(document.getElementById('max-tokens-input').value || '50');
    const temperature = parseFloat(document.getElementById('temperature-input').value || '0.7');

    let generated = [...inputIds];
    for (let t=0;t<maxTokens;t++) {
      const inp = tf.tensor2d([generated.slice(-seqLen)], [1, seqLen], 'int32');
      const encOut = this.encoder.apply(inp); // [1, seqLen, dModel]
      const last = encOut.slice([0, seqLen-1, 0],[1,1,this.encoder.dModel]).reshape([this.encoder.dModel]);
      const logits = tf.matMul(last.expandDims(0), this.projectionW).reshape([window.state.vocabSize+1]);
      const probs = tf.softmax(logits.div(tf.scalar(temperature)));
      const probsData = await probs.data();
      // sample
      let r = Math.random(), accum=0, idx=0;
      for (let i=0;i<probsData.length;i++){ accum += probsData[i]; if (r <= accum) { idx = i; break; } }
      if (idx === 0) { // PAD token or eos-like
        break;
      }
      generated.push(idx);
      inp.dispose(); encOut.dispose(); last.dispose(); logits.dispose(); probs.dispose();
      await tf.nextFrame();
    }
    const end = performance.now();
    window.state.latency = `${(end - start).toFixed(2)} ms`;
    document.getElementById('metric-latency').innerText = window.state.latency;
    return this.tokenizer.decode(generated.slice(-maxTokens));
  }

  async saveModel() {
    alert('Saving model: embeddings and projection weights will be downloaded.');
    // Save embedding and projection as JSON/array buffers
    // For brevity, save embedding matrix as .json
    const emb = await this.encoder.getEmbeddingWeights()[0].array();
    const proj = await this.projectionW.array();
    const payload = { embedding: emb, projection: proj };
    const blob = new Blob([JSON.stringify(payload)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a'); a.href = url; a.download = 'transformer_model.json'; document.body.appendChild(a); a.click(); a.remove();
    URL.revokeObjectURL(url);
  }

  async loadModel() {
    alert('Load model not implemented automatically. Use browser TF.js IO handlers or paste weights manually.');
  }
}

/* ======================
   Chart helper
   ====================== */
let lossChart = null;
function initLossChart() {
  const ctx = document.getElementById('lossChart').getContext('2d');
  if (lossChart) lossChart.destroy();
  lossChart = new Chart(ctx, {
    type: 'line',
    data: { labels: [], datasets: [{ label: 'Training Loss', data: [], fill: true, tension: 0.3 }] },
    options: { responsive: true, plugins: { legend: { display: false } }, scales: { y: { beginAtZero: true } } }
  });
}

/* ======================
   UI wiring & public functions
   ====================== */
window.tokenizer = new Tokenizer();
window.dataHandler = window.dataHandler || new DataHandler(window.tokenizer);
window.miniTransformer = new MiniTransformer(window.tokenizer, window.dataHandler);
window.generateTextFromModel = async function() {
  const prompt = document.getElementById('prompt-input').value || '';
  const out = document.getElementById('llm-output');
  out.innerText = 'Generating...';
  let result = '';
  if (window.state.mode === 'MARKOV') {
    result = window.markovChain.generate(prompt);
  } else {
    result = await window.miniTransformer.generate(prompt);
  }
  out.innerText = result;
};

/* AWS modal handlers */
function showAwsConfigModal() { document.getElementById('aws-config-modal').style.display = 'flex'; }
function closeAwsConfigModal() { document.getElementById('aws-config-modal').style.display = 'none'; }

window.connectAwsFromModal = async function() {
  const region = document.getElementById('aws-region').value.trim();
  const accessKeyId = document.getElementById('aws-access-key').value.trim();
  const secretAccessKey = document.getElementById('aws-secret-key').value.trim();
  const sessionToken = document.getElementById('aws-session-token').value.trim();
  const bucket = document.getElementById('aws-bucket').value.trim();
  const wsUrl = document.getElementById('aws-ws-url').value.trim();
  const roleArn = document.getElementById('aws-role-arn').value.trim();
  const imageUri = document.getElementById('aws-image-uri').value.trim();
  const instanceType = document.getElementById('aws-instance-type').value.trim();

  if (region) AwsRuntimeConfig.region = region;
  AwsRuntimeConfig.accessKeyId = accessKeyId || null;
  AwsRuntimeConfig.secretAccessKey = secretAccessKey || null;
  AwsRuntimeConfig.sessionToken = sessionToken || null;
  if (bucket) AwsRuntimeConfig.bucket = bucket;
  if (wsUrl) AwsRuntimeConfig.wsUrl = wsUrl;
  if (roleArn) AwsRuntimeConfig.roleArn = roleArn;
  if (imageUri) AwsRuntimeConfig.sagemakerImageUri = imageUri;
  if (instanceType) AwsRuntimeConfig.instanceType = instanceType;

  closeAwsConfigModal();
  logDev('AWS runtime config updated (temporary).', AwsRuntimeConfig);
  document.getElementById('cloud-flag').innerText = String(window.USE_CLOUD_TRAINING);
  if (AwsRuntimeConfig.wsUrl && !AwsRuntimeConfig.wsUrl.includes('API_GATEWAY_WS_URL')) {
    AwsMetrics.connect(AwsRuntimeConfig.wsUrl);
    AwsMetrics.onMetric(processMetricEvent);
  }
};

/* Start cloud training (public) */
window.startCloudTraining = async function() {
  if (!window.USE_CLOUD_TRAINING) { alert('Cloud training disabled. Toggle USE_CLOUD_TRAINING in Dev Panel.'); return; }
  if (!window.dataHandler || window.dataHandler.tokenIDs.length < 10) { alert('Tokenize data before starting remote job.'); return; }
  const runId = `run-${Date.now()}`;
  const trainConfig = {
    epochs: parseInt(document.getElementById('epochs-input').value || '6'),
    batchSize: parseInt(document.getElementById('batch-size-input').value || '8'),
    learningRate: parseFloat(document.getElementById('learning-rate-input').value || '0.0005'),
    seqLen: 128,
    vocabSize: window.state.vocabSize,
    inputS3Uri: `s3://${AwsRuntimeConfig.bucket}/${AwsRuntimeConfig.projectName}/${runId}/training/`,
    outputS3Uri: `s3://${AwsRuntimeConfig.bucket}/${AwsRuntimeConfig.projectName}/${runId}/output/`,
    instanceType: AwsRuntimeConfig.instanceType || DEFAULT_INSTANCE_TYPE_PLACEHOLDER,
    instanceCount: 1,
    quantize: false
  };

  try {
    // NOTE: For production, pre-upload training data to S3 or provide API to handle upload.
    const res = await AwsTraining.startSageMakerTraining(runId, trainConfig);
    alert(`Requested remote training job: ${res.trainingJobName}`);
    if (AwsRuntimeConfig.wsUrl && !AwsRuntimeConfig.wsUrl.includes('API_GATEWAY_WS_URL')) {
      AwsMetrics.connect(AwsRuntimeConfig.wsUrl);
      AwsMetrics.onMetric(processMetricEvent);
    }
  } catch (err) {
    console.error(err);
    alert('Failed to start cloud training: ' + (err.message || String(err)));
  }
};

/* Metric event handler */
function processMetricEvent(event) {
  if (!event || !event.type) return;
  if (event.type === 'metrics') {
    const epoch = event.epoch || event.step || (lossChart?.data?.labels?.length || 0) + 1;
    const loss = event.loss || 0;
    window.state.currentLoss = loss;
    if (lossChart) {
      lossChart.data.labels.push(epoch);
      lossChart.data.datasets[0].data.push(loss);
      lossChart.update('none');
    }
    document.getElementById('metric-loss').innerText = (typeof loss === 'number') ? loss.toFixed(4) : loss;
    document.getElementById('metric-epochs').innerText = String(event.epoch || window.state.epochsRun || '');
  } else if (event.type === 'status') {
    const stateMsg = event.state || event.message || '';
    document.getElementById('training-progress-text').innerText = `${stateMsg} ${event.message || ''}`;
    if (event.state === 'COMPLETED' || event.state === 'FAILED') {
      document.getElementById('training-status').classList.remove('active');
      window.state.isTraining = false;
    } else if (event.state === 'STARTING' || event.state === 'RUNNING' || event.state === 'CONNECTED') {
      document.getElementById('training-status').classList.add('active');
      window.state.isTraining = true;
    }
  }
}
AwsMetrics.onMetric(processMetricEvent);

/* ======================
   Test harness and dev helpers
   ====================== */
function openTestHarness() { document.getElementById('test-harness-modal').style.display = 'flex'; }
document.getElementById('close-test').addEventListener('click', ()=>document.getElementById('test-harness-modal').style.display='none');

document.getElementById('run-local-test').addEventListener('click', async ()=>{
  const log = document.getElementById('test-log');
  log.textContent = 'Running local Transformer training test...\n';
  loadSampleData();
  await new Promise(r=>setTimeout(r,500));
  dataHandler.tokenizeAndBuildMatrix();
  document.getElementById('epochs-input').value = '1';
  await miniTransformer.trainOnText();
  log.textContent += `Local training finished. Loss: ${window.state.currentLoss}\n`;
});

document.getElementById('run-cloud-mock-test').addEventListener('click', async ()=>{
  const log = document.getElementById('test-log');
  log.textContent = 'Emitting mocked cloud metrics...\n';
  const mock = [
    {type:'status', state:'STARTING', message:'Mock starting', timestamp:new Date().toISOString()},
    {type:'metrics', step:1, epoch:1, loss:0.98, lr:0.0005, timestamp:new Date().toISOString()},
    {type:'metrics', step:2, epoch:2, loss:0.6, lr:0.0005, timestamp:new Date().toISOString()},
    {type:'status', state:'COMPLETED', message:'Mock done', timestamp:new Date().toISOString()}
  ];
  mock.forEach((m,i)=> setTimeout(()=> { AwsMetrics.appendMetric(m); log.textContent += `Emitted: ${JSON.stringify(m)}\n`; }, i*600) );
});

/* Dev panel toggles */
window.toggleCloudMode = function() {
  window.USE_CLOUD_TRAINING = !window.USE_CLOUD_TRAINING;
  document.getElementById('cloud-flag').innerText = String(window.USE_CLOUD_TRAINING);
  document.getElementById('start-cloud-btn').style.display = (window.USE_CLOUD_TRAINING && window.state.mode==='TRANSFORMER') ? 'inline-block' : 'none';
  logDev('USE_CLOUD_TRAINING now', window.USE_CLOUD_TRAINING);
};

function emitMockMetrics() {
  const now = new Date().toISOString();
  const m = { type:'metrics', step: Math.floor(Math.random()*100), epoch: Math.floor(Math.random()*10), loss: Math.random(), lr: 0.0005, timestamp: now };
  AwsMetrics.appendMetric(m);
  logDev('Emitted mock metric', m);
}

/* Download helper for check_no_aws_keys.sh */
function downloadCheckNoAwsKeys() {
  const content = `#!/usr/bin/env bash
set -euo pipefail
echo "Scanning for AWS key-like patterns..."
if grep -IEnr --perl-regexp 'AKIA[0-9A-Z]{16}|ASIA[0-9A-Z]{16}' . || true; then
  echo "Potential AWS Access Key IDs found. Aborting."
  exit 1
fi
if grep -IEnr --perl-regexp '(?i)aws_secret_access_key' . || true; then
  echo "Potential AWS secret key literal found. Aborting."
  exit 1
fi
echo "No obvious AWS key patterns found."
exit 0
`;
  const blob = new Blob([content], { type: 'text/x-sh' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a'); a.href = url; a.download = 'check_no_aws_keys.sh'; document.body.appendChild(a); a.click(); a.remove();
  URL.revokeObjectURL(url);
}

/* Sample data loader */
function loadSampleData() {
  const sample = `In the beginning was the word. The quick brown fox jumped over the lazy dog. This is a small sample corpus used for in-browser transformer training experiments. TensorFlow.js allows training lightweight models directly in your browser.`;
  document.getElementById('text-input').value = sample;
  document.getElementById('file-preview').innerHTML = '';
  document.getElementById('file-analytics-summary').innerText = 'Sample dataset loaded';
  window.dataHandler.corpusText = sample;
}

/* ======================
   Initialization
   ====================== */
(function init() {
  document.getElementById('max-tokens-input').addEventListener('input', (e)=>CONFIG.maxTokens = parseInt(e.target.value || '50'));
  document.getElementById('temperature-input').addEventListener('input', (e)=>CONFIG.temperature = parseFloat(e.target.value || '0.7'));
  document.getElementById('embedding-size-input').addEventListener('input', (e)=>CONFIG.embeddingDim = parseInt(e.target.value || '128'));
  document.getElementById('num-heads-input').addEventListener('input', (e)=>{}); // bound element present
  document.getElementById('epochs-input').addEventListener('input', (e)=>{ CONFIG.train.epochs = parseInt(e.target.value || '6'); });
  document.getElementById('batch-size-input').addEventListener('input', (e)=>{ CONFIG.train.batchSize = parseInt(e.target.value || '8'); });
  document.getElementById('learning-rate-input').addEventListener('input', (e)=>{ CONFIG.train.learningRate = parseFloat(e.target.value || '0.0005'); });

  document.getElementById('model-mode-toggle').checked = (window.state.mode === 'TRANSFORMER');
  document.getElementById('cloud-flag').innerText = String(window.USE_CLOUD_TRAINING);
  initLossChart();

  // Expose useful objects to window for debugging
  window.AwsTraining = AwsTraining;
  window.AwsMetrics = AwsMetrics;
  window.AwsRuntimeConfig = AwsRuntimeConfig;
  window.downloadCheckNoAwsKeys = downloadCheckNoAwsKeys;

  // Hook metrics bus
  AwsMetrics.onMetric(processMetricEvent);

  logDev('App initialized.');
})();

/* Minimal CONFIG (keeps original public fields) */
window.CONFIG = {
  modelMode: 'MARKOV',
  maxTokens: 50,
  temperature: 0.7,
  embeddingDim: 128,
  ffnDim: 256,
  train: { epochs: 6, batchSize: 8, learningRate: 0.0005 }
};

  </script>
</body>
</html>
